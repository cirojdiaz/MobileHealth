{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7--TR3FROBa6"
   },
   "source": [
    "# Early stage ANN Model to predict HR\n",
    "Purpose:\n",
    "-  Building ANN (Artificial Neural Network) Model using 4 health metrics to predicit HR\n",
    "- Using four Variables to predict one (Many to one)\n",
    "\n",
    "\n",
    "Notes:\n",
    "- Data was downsampled\n",
    "- This model is based of the three variables model which is done by the Summer 2022 team \n",
    "- The Four-Variable ANN Model (All Variables) is the updated version of this model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EqVNFgFP-87"
   },
   "source": [
    "# Loading and filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24369,
     "status": "ok",
     "timestamp": 1671540971444,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "u5SRnIEuJOez",
    "outputId": "9f186d4f-f117-4092-f1b0-b89584a279a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wfdb in c:\\users\\vithu\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: SoundFile<0.12.0,>=0.10.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (0.11.0)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.2.2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (3.5.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.10.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (1.21.5)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (1.9.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.8.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (2.28.1)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (1.4.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from pandas<2.0.0,>=1.0.0->wfdb) (2022.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.8.1->wfdb) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.8.1->wfdb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.8.1->wfdb) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.8.1->wfdb) (2022.9.14)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from SoundFile<0.12.0,>=0.10.0->wfdb) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from cffi>=1.0->SoundFile<0.12.0,>=0.10.0->wfdb) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.2.2->wfdb) (1.16.0)\n",
      "Requirement already satisfied: tensorflow_addons in c:\\users\\vithu\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorflow_addons) (21.3)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from packaging->tensorflow_addons) (3.0.9)\n",
      "Requirement already satisfied: keras-tuner in c:\\users\\vithu\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from keras-tuner) (2.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from keras-tuner) (1.21.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from keras-tuner) (21.3)\n",
      "Requirement already satisfied: ipython in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from keras-tuner) (7.31.1)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from keras-tuner) (1.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from keras-tuner) (2.28.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (63.4.1)\n",
      "Requirement already satisfied: pygments in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (2.11.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (3.0.20)\n",
      "Requirement already satisfied: decorator in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from packaging->keras-tuner) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (1.26.11)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (1.51.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (2.15.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (3.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.37.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (3.20.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (1.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (5.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython->keras-tuner) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    " # install external libraries\n",
    "\n",
    "!pip install wfdb\n",
    "!pip install tensorflow_addons\n",
    "!pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XcRqAeQhJTap"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import time\n",
    "import wfdb\n",
    "from wfdb import processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29115,
     "status": "ok",
     "timestamp": 1671541034290,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "liUXtTUNmSTl",
    "outputId": "d0e50903-e546-4960-b9df-68721b4d95cc"
   },
   "outputs": [],
   "source": [
    "#Need to allow code to access the CSV Dataset. \n",
    "#Setting up an environment variable. Will add a readme file in order to show how to use this method.\n",
    "\n",
    "\n",
    "csv_folder_path = os.environ.get(\"CSV_Folder_Path\")\n",
    "file_path = os.path.join(csv_folder_path, \"uq_vsd_case04_alldata.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3984,
     "status": "ok",
     "timestamp": 1671541041316,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "-vNLUIMEmTcs",
    "outputId": "25d9a840-7221-461b-b6d4-3e4a838d8d0c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4999,
     "status": "ok",
     "timestamp": 1671541049735,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "VtnZht98mX97",
    "outputId": "007eed9b-4a77-4cbf-acec-6006efbfdde9"
   },
   "outputs": [],
   "source": [
    "# FILTERING THE DATA\n",
    "df = pd.read_csv(\"uq_vsd_case04_alldata.csv\", index_col=False, error_bad_lines=False, usecols=['Time', 'HR', 'SpO2', 'NBP (Mean)', 'ECG'])\n",
    "\n",
    "df = df.dropna() #drop rows with NaN\n",
    "\n",
    "case4_filteredData = df.loc[(~df['HR'].isnull()) & (~df['SpO2'].isnull()) & (~df['NBP (Mean)'].isnull()) & (~df['ECG'].isnull())]\n",
    "case4_filteredData.drop('Time', axis=1, inplace=True)\n",
    "print(case4_filteredData.head(10))\n",
    "print(case4_filteredData.tail(10))\n",
    "print(case4_filteredData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 4089,
     "status": "ok",
     "timestamp": 1671541060807,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "GPIHnjvtewhv",
    "outputId": "1502647a-1496-4efd-805c-abf98da28f49"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'new_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26236\\3299916616.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# plot data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"new_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# specify columns to plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'new_data.csv'"
     ]
    }
   ],
   "source": [
    "# plot data\n",
    "dataset = pd.read_csv(\"new_data.csv\", index_col=0, header=0)\n",
    "values = dataset.values\n",
    "\n",
    "# specify columns to plot\n",
    "groups = [0,1,2,3]\n",
    "i =1\n",
    "for g in groups:\n",
    "  plt.subplot(len(groups), 1, i)\n",
    "  plt.plot(values[:, g])\n",
    "  plt.title(dataset.columns[g], y=0.5, loc='right')\n",
    "  i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93940,
     "status": "ok",
     "timestamp": 1671541174117,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "79RiXJ0YJPV_",
    "outputId": "67ab037f-fef1-418d-e87b-c6ad6687738e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'case4_filteredData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26236\\687391617.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdf_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mold_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcase4_filteredData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Size of old data:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'case4_filteredData' is not defined"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------- MANUALLY RESAMPLE -------------------------------------------------------------------\n",
    "def downsample_data(window):\n",
    "  df_temp = pd.DataFrame(columns=['HR', 'SpO2', 'NBP (Mean)', 'ECG'])\n",
    "  HR_sum = 0\n",
    "  SPO_sum = 0\n",
    "  NBP_sum = 0\n",
    "  ECG_sum = 0\n",
    "\n",
    "  for i in range(1, len(values) - 1):\n",
    "    HR_sum += values[i, 0]\n",
    "    SPO_sum += values[i, 1]\n",
    "    NBP_sum += values[i, 2]\n",
    "    ECG_sum += values[i, 3]\n",
    "    if (i + 1) % window == 0:\n",
    "      HR_sum = HR_sum/window\n",
    "      SPO_sum = SPO_sum/window\n",
    "      NBP_sum = NBP_sum/window\n",
    "      ECG_sum = ECG_sum/window\n",
    "      row = {'HR':HR_sum, 'SpO2':SPO_sum, 'NBP (Mean)':NBP_sum, 'ECG':ECG_sum}\n",
    "      df_temp = df_temp.append(row, ignore_index = True)\n",
    "      HR_sum = 0\n",
    "      SPO_sum = 0\n",
    "      NBP_sum = 0\n",
    "      ECG_sum = 0\n",
    "  return df_temp\n",
    "\n",
    "old_data = case4_filteredData\n",
    "print('Size of old data:')\n",
    "print(old_data.shape)\n",
    "\n",
    "# downsampling data\n",
    "case4_filteredData = downsample_data(15)\n",
    "\n",
    "print('Size of new data:')\n",
    "print(case4_filteredData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1671541248714,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "YZQkou8mnHjE",
    "outputId": "7d2d151e-a399-4773-cbe9-15556b96b5d3"
   },
   "outputs": [],
   "source": [
    "# plot data\n",
    "dataset = case4_filteredData # use if google drive\n",
    "values = dataset.values\n",
    "\n",
    "# specify columns to plot\n",
    "groups = [0,1,2,3]\n",
    "i =1\n",
    "for g in groups:\n",
    "  plt.subplot(len(groups), 1, i)\n",
    "  plt.plot(values[:, g])\n",
    "  plt.title(dataset.columns[g], y=0.5, loc='right')\n",
    "  i += 1\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMUm6UQKGw-T"
   },
   "source": [
    "# Pre-processing - Combining Data, Scaling, Splitting Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3B_gv4Q6G_-P"
   },
   "outputs": [],
   "source": [
    "# Function for splitting data into training and test set non-randomly\n",
    "\n",
    "# This is from the 3 variables model \n",
    "\n",
    "\"\"\"\n",
    "This functions splits the data into training and test sets.\n",
    "\n",
    "@param data_df : a Pandas Dataframe that contains the data to be split into training and test sets\n",
    "@param train_size : a double/float that is within the range of 0 and 1 and represents the fraction of the data to be used for the training set\n",
    "\n",
    "@returns two Numpy arrays that represent the training and test sets, respectively\n",
    "\"\"\"\n",
    "def split_data_train_test(data_df, train_size):\n",
    "  train_df, test_df = data_df[0:round(train_size * len(data_df)), :], data_df[round(train_size * len(data_df)):len(data_df), :]\n",
    "\n",
    "  return train_df, test_df\n",
    "\n",
    "# Function for scaling the data\n",
    "\"\"\"\n",
    "This function takes in the data to be scaled and uses Min-Max Scaling.\n",
    "\n",
    "@param data : a Numpy array that represents the inputted data to be scaled\n",
    "\n",
    "@returns the scaled data as a Numpy array, the MinMax scaler object, the scaling factor used, and the minimum value of the original inputted data\n",
    "\"\"\"\n",
    "def minMaxScaling(data):\n",
    "  scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0.1, 1))\n",
    "  scaler.fit(data)\n",
    "\n",
    "  scaleFactor = scaler.scale_\n",
    "  data_min = scaler.data_min_\n",
    "  data_max = scaler.data_max_\n",
    "  data = scaler.transform(data)\n",
    "\n",
    "  return data, scaler, scaleFactor, data_min, data_max\n",
    "\n",
    "\"\"\"\n",
    "This function takes in the data to be scaled and uses Standard Scaling.\n",
    "\n",
    "@param data : a Numpy array that represents the inputted data to be scaled\n",
    "\n",
    "@returns the scaled data as a Numpy array, the Standard scaler object, the mean of the original inputted data, and the standard deviation of the original inputted data\n",
    "\"\"\"\n",
    "def stdScaling(data):\n",
    "  scaler = sklearn.preprocessing.StandardScaler()\n",
    "  scaler.fit(data)\n",
    "\n",
    "  mean = scaler.mean_\n",
    "  std_dev = scaler.var_ ** 0.5\n",
    "  data = scaler.transform(data)\n",
    "\n",
    "  return data, scaler, mean, std_dev\n",
    "\n",
    "\"\"\"\n",
    "This function plots the inputted data in a 2D scatterplot.\n",
    "\n",
    "@param xvals : a 1D Numpy array that contains the data to be plotted on the x-axis\n",
    "@param data : a 1D Numpy array that contains the data to be plotted on the y-axis\n",
    "@param xVariable : a string that represents the label of the data to be plotted for x-axis\n",
    "@param yVariable : a string that represents the label of the data to be plotted for y-axis\n",
    "\"\"\"\n",
    "def plot_data(xvals, data, xVariable, yVariable):\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.scatter(xvals, data)\n",
    "  plt.title(\"Plot of Data for \" + yVariable + \", \" + xVariable)\n",
    "  plt.ylabel(yVariable)\n",
    "  plt.xlabel(xVariable)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1671541253502,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "NEwz2L1lHNlM",
    "outputId": "2fdf268b-bd70-4a1c-af98-c01cd4d9be8e"
   },
   "outputs": [],
   "source": [
    "# Scaling and splitting dataset, using R-Wave Voltages and SpO2 as features\n",
    "pd.set_option(\"max_rows\", 1000)\n",
    "print(case4_filteredData.head(20))\n",
    "\n",
    "case4_filteredData_arr_scaled, minMaxScaler, minMaxScaleFactor, case4_filteredData_min, case4_filteredData_max = minMaxScaling(case4_filteredData)\n",
    "# combinedData_arr_scaled, stdScaler, combinedData_mean, combinedData_stdDev = stdScaling(combinedData_df) # uncomment this if standard scaling will be used\n",
    "\n",
    "# Add more columns names if more health variables are added.\n",
    "case4_filteredData_scaled = pd.DataFrame(case4_filteredData_arr_scaled, columns=[\"HR\", \"SpO2\",\"NBP\",\"ECG\"])\n",
    "\n",
    "print()\n",
    "print(case4_filteredData_scaled[0:20])\n",
    "\n",
    "listOfVariables = [\"HR\", \"SpO2\",\"NBP\",\"ECG\"] # Add more columns names if more health variables are added.\n",
    "listOfVariables_len = len(listOfVariables) - 1 \n",
    "\n",
    "data_train, data_test = split_data_train_test(np.array(case4_filteredData_scaled[listOfVariables]), train_size=0.8) # scaled data\n",
    "# data_train, data_test = split_data_train_test(np.array(combinedData_df[listOfVariables]), train_size=0.8) # non-scaled\n",
    "\n",
    "print(\"\\nOriginal data train and test: \")\n",
    "print(data_train[0:20])\n",
    "\n",
    "# print(\" Original test\")\n",
    "print(data_test[0:20])\n",
    "\n",
    "\n",
    "print(\" Here to see the size of each data: \")\n",
    "print(\"This is the size of the train data: \")\n",
    "print(data_train.shape)\n",
    "print(\"This is the size of the test data: \")\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1671541258715,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "wYj2ZOz-FHEH",
    "outputId": "cb112c93-7bee-4b1d-83d1-7534fd302357"
   },
   "outputs": [],
   "source": [
    "# Splitting data into previous values and next values\n",
    "# From the 3 variables model.\n",
    "\"\"\"\n",
    "This function splits the training and test sets and turns a certain number of previous values as \"features\" and the next value as the \"target\"\n",
    "\n",
    "@param train_array : a Numpy array with at least 2 columns of the training set values\n",
    "@param test_array : a Numpy array with at least 2 columns of the test set values\n",
    "@param numOfPrevValues : an integer that represents the number of previous rows of values to use as features\n",
    "@param targetIndex : an integer that represents the column number to use for target values\n",
    "\n",
    "@returns four Numpy arrays for the previous values in the training set, the next 1 value in the training set, the previous values\n",
    "         in the test set, and the next 1 value in the test set\n",
    "\"\"\"\n",
    "def splitDataToFeaturesTargetValues(train_array, test_array, numOfPrevValues=27, targetIndex=1):\n",
    "\n",
    "  prevData_train = []\n",
    "  nextData_train = [] \n",
    "  prevData_test = []\n",
    "  nextData_test = []\n",
    "\n",
    "  for i in range(numOfPrevValues, len(train_array)):\n",
    "    prevData_train.append(train_array[i-numOfPrevValues: i])\n",
    "    nextData_train.append(train_array[i][targetIndex])\n",
    "\n",
    "  for j in range(numOfPrevValues, len(test_array)):\n",
    "    prevData_test.append(test_array[j-numOfPrevValues: j])\n",
    "    nextData_test.append(test_array[j][targetIndex])\n",
    "  \n",
    "  prevData_train = np.array(prevData_train)\n",
    "  nextData_train = np.array(nextData_train).reshape(len(nextData_train), 1)\n",
    "  prevData_test = np.array(prevData_test)\n",
    "  nextData_test = np.array(nextData_test).reshape(len(nextData_test), 1)\n",
    "\n",
    "  return prevData_train, nextData_train, prevData_test, nextData_test\n",
    "\n",
    "data_train_features, data_train_label, data_test_features, data_test_label = splitDataToFeaturesTargetValues(data_train, data_test, numOfPrevValues=50, targetIndex=1)\n",
    "print(\"This is the data_train_features\")\n",
    "print(data_train_features[0:10])\n",
    "print(\"This is the data_train_label\")\n",
    "print(data_train_label[0:10])\n",
    "print(\"This is the data_test_features\")\n",
    "print(data_test_features[0:10])\n",
    "print(\"This is the data_test_label\")\n",
    "print(data_test_label[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJq5puPtFJHE"
   },
   "source": [
    "###General Functions for Machine Learning Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSztDJHUFOGg"
   },
   "outputs": [],
   "source": [
    "# Function that makes the loss function plots and linear regression model\n",
    "# From the 3 Variables Model \"Mostly not needed\"\n",
    "\n",
    "\"\"\"\n",
    "This function plots a loss function based on the output of the model fitting.\n",
    "\n",
    "@param loss_vals : a Tensorflow History object from model fitting, which is used to get the training and validation loss values\n",
    "@param startRange : an integer that is >= 0, is within the indices of loss_vals, and represents the start indices of the loss values to plot\n",
    "@param endRange : an integer that is <= len(loss_vals) - 1, is larger than startRange, and represents the end indices of the loss values to plot\n",
    "\"\"\"\n",
    "def plot_loss(loss_vals, startRange, endRange):\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plt.plot(loss_vals.history['loss'][startRange:endRange], label=\"Training Loss\") \n",
    "  plt.plot(loss_vals.history['val_loss'][startRange:endRange], label=\"Validation Loss\") \n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Error\")\n",
    "  plt.title(\"Learning Curve\")\n",
    "  plt.legend()\n",
    "\n",
    "\"\"\"\n",
    "This function plots the inputted actual and predicted data in a 2D scatterplot.\n",
    "\n",
    "@param xvals : a 1D Numpy array that contains the data to be plotted on the x-axis\n",
    "@param actual_data : a 1D Numpy array that contains the actual data to be plotted on the y-axis\n",
    "@param predicted_data : a 1D Numpy array that contains the predicted data to be plotted on the y-axis\n",
    "@param xName : a string that represents the x-values and label for the x-axis\n",
    "@param yName : a string that represents the y-values and label for the y-axis\n",
    "\"\"\"\n",
    "def plot_model_data(xvals, actual_data, predicted_data, xName, yName):\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plt.scatter(xvals, actual_data, label=\"Actual Data\", color=\"r\")\n",
    "  plt.plot(xvals, predicted_data, label=\"Predicted Data\", color=\"black\")\n",
    "  plt.title(\"Plot of Model Data for \" + xName + \" Against \" + yName)\n",
    "  plt.xlabel(xName)\n",
    "  plt.ylabel(yName)\n",
    "  plt.legend()\n",
    "\n",
    "\"\"\"\n",
    "This function plots the inputted actual and predicted data in a 3D scatterplot.\n",
    "\n",
    "@param x_val : a 1D Numpy array that contains the data to be plotted on the x-axis\n",
    "@param y_val : a 1D Numpy array that contains the data to be plotted on the y-axis\n",
    "@param actual_data : a 1D Numpy array that contains the actual data to be plotted on the z-axis\n",
    "@param predicted_data : a 1D Numpy array that contains the predicted data to be plotted on the z-axis\n",
    "@param xName : a string that represents the x-values and label for the x-axis\n",
    "@param yName : a string that represents the y-values and label for the y-axis\n",
    "@param zName : a string that represents the z-values and label for the z-axis\n",
    "\"\"\"\n",
    "def plot_model_data3D(x_val, y_val, actual_data, predicted_data, xName, yName, zName):\n",
    "  fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "  ax = plt.axes(projection='3d')\n",
    "  ax.scatter3D(x_val, y_val, actual_data, label='Actual Data', color=\"red\")\n",
    "  ax.plot3D(x_val, y_val, predicted_data, label='Predicted Data', color=\"black\")\n",
    "\n",
    "  ax.set_title(\"Plot of Model Data for \" + xName + \" And \" + yName + \" Against \" + zName, pad=20)\n",
    "  ax.set_xlabel(xName)\n",
    "  ax.set_ylabel(yName)\n",
    "  ax.set_zlabel(zName)\n",
    "  ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYfukzbtF75O"
   },
   "outputs": [],
   "source": [
    "# Functions for getting metrics and plotting predictions\n",
    "# From the 3 variables model\n",
    "\n",
    "\"\"\"\n",
    "This functions gets and prints the root mean squared error (RMSE) of the targets.\n",
    "\n",
    "@param test_targets : a Numpy array that represents the actual target values\n",
    "@param predicted_targets : a Numpy array that represents the predicted target values\n",
    "\n",
    "@returns the RMSE double/float value\n",
    "\"\"\"\n",
    "def get_and_print_rmse_for_model(test_targets, predicted_targets):\n",
    "  rmseMetric = tf.keras.metrics.RootMeanSquaredError()\n",
    "  rmseMetric.update_state([test_targets], [predicted_targets])\n",
    "  rmseMetricResult = rmseMetric.result().numpy()\n",
    "\n",
    "  print(\"RMSE: \" + str(rmseMetricResult))\n",
    "  return rmseMetricResult\n",
    "\n",
    "\"\"\"\n",
    "This functions gets and prints the coefficient of determination (R^2) of the targets.\n",
    "\n",
    "@param test_targets : a Numpy array that represents the actual target values\n",
    "@param predicted_targets : a Numpy array that represents the predicted target values\n",
    "\n",
    "@returns the R^2 double/float value\n",
    "\"\"\"\n",
    "def get_and_print_R2_for_model(test_targets, predicted_targets):\n",
    "  r2 = sklearn.metrics.r2_score(y_true=test_targets, y_pred=predicted_targets)\n",
    "  print(\"R2 (Sklearn): \" + str(r2))\n",
    "  \n",
    "  return r2\n",
    "\n",
    "\"\"\"\n",
    "This function uses the mean absolute percentage error (MAPE) to get a percentage error for a regression model. \n",
    "\n",
    "Note that if the output is a number above 1 or below 0, usually very big in magnitude, it is because \n",
    "some of the numbers in the test_targets are 0 or very close to 0, which gives a division by 0.\n",
    "\n",
    "@param test_targets : a Numpy array that represents the actual target values\n",
    "@param predicted_targets : a Numpy array that represents the predicted target values\n",
    "\n",
    "@returns the MAPE double/float value which represents the decimal form of the percentage, so generally between 0 and 1\n",
    "\"\"\"\n",
    "def get_and_print_mape_for_model(test_targets, predicted_targets):\n",
    "  mape = sklearn.metrics.mean_absolute_percentage_error(test_targets, predicted_targets)\n",
    "  print(\"MAPE: \" + str(mape))\n",
    "\n",
    "  return mape\n",
    "\n",
    "\"\"\"\n",
    "This function gets the mean absolute error (MAE) for a regression model. \n",
    "\n",
    "Note that if the output is a number below 0, usually very big in magnitude, it is because \n",
    "some of the numbers in the test_targets are 0 or very close to 0, which gives a division by 0.\n",
    "\n",
    "@param test_targets : a Numpy array that represents the actual target values\n",
    "@param predicted_targets : a Numpy array that represents the predicted target values\n",
    "\n",
    "@returns the MAPE double/float value, which should be greater than or equal to 0\n",
    "\"\"\"\n",
    "def get_and_print_mae_for_model(test_targets, predicted_targets):\n",
    "  mae = sklearn.metrics.mean_absolute_error(test_targets, predicted_targets)\n",
    "  print(\"MAE: \" + str(mae))\n",
    "\n",
    "  return mae\n",
    "\n",
    "\"\"\"\n",
    "This functions plots the actual and predicted values as a line plot.\n",
    "\n",
    "@param indices : the x-values of the plot as a 1-column Numpy array\n",
    "@param test_targets : a 1-column Numpy array that represents the actual target values\n",
    "@param predicted_targets : a 1-column Numpy array that represents the predicted target values\n",
    "\"\"\"\n",
    "def plot_actual_and_predictions_line(x_val, test_targets, predicted_targets):\n",
    "  fig = plt.figure(figsize=(12, 8))\n",
    "  plt.plot(x_val, test_targets, label=\"Actual Values\", c=\"b\")\n",
    "  plt.plot(x_val, predicted_targets, label=\"Predicted Values\", c=\"r\")\n",
    "\n",
    "  plt.ylabel(\"Values\")\n",
    "  plt.title(\"Plot of Actual and Predicted Values\")\n",
    "  plt.legend()\n",
    "\n",
    "\"\"\"\n",
    "This functions plots the actual and predicted values as a scatter plot.\n",
    "\n",
    "@param indices : a 1-column Numpy array that represents x-values of the plot \n",
    "@param test_targets : a 1-column Numpy array that represents the actual target values\n",
    "@param predicted_targets : a 1-column Numpy array that represents the predicted target values\n",
    "\"\"\"\n",
    "def plot_actual_and_predictions_scatter(x_val, test_targets, predicted_targets):\n",
    "  fig = plt.figure(figsize=(12, 8))\n",
    "  plt.scatter(x_val, test_targets, label=\"Actual Values\", c=\"b\")\n",
    "  plt.scatter(x_val, predicted_targets, label=\"Predicted Values\", c=\"r\")\n",
    "\n",
    "  plt.ylabel(\"Value\")\n",
    "  plt.title(\"Plot of Actual and Predicted Values\")\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1671541280998,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "rogc3tcTF0SK",
    "outputId": "ae6009e0-9d23-4481-cc38-0407f5ce5145"
   },
   "outputs": [],
   "source": [
    "# Functions for making artificial neural networks with a different amount of layers\n",
    "# From the 3 variables model\n",
    "\"\"\"\n",
    "This function makes a custom artificial neural network model.\n",
    "\n",
    "@param train_features : a Numpy array that represents the training data features\n",
    "@param train_labels : a Numpy array that represents the training data targets\n",
    "@param numOfUnits : a list that contains the number of units for each layer, which could be customized by providing different integers in the desired order. \n",
    "                    The size of the list should either be equal to 1 or numOfLayers. If size = 1, then all layers will have that number of units in the list.\n",
    "                    (eg. [4, 8, 16] if numOfLayers = 3, or [4] for any value of numOfLayers)\n",
    "@param numOfLayers : an integer that represents the desired of layers to add to the model \n",
    "@param epochNum : the number of iterations/epochs for running the model\n",
    "@param learning_rate : a double/float that represents the learning rate of the model\n",
    "\n",
    "@returns the artificial neural network TensorFlow model and the result from fitting the model\n",
    "\n",
    "\"\"\"\n",
    "def make_custom_ANN(train_features, train_labels, numOfUnits=[4], numOfLayers=3, batchNum=None, epochNum=100, learning_rate=0.01, min_delta=0.0007):\n",
    "  callback_valLoss = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, min_delta=min_delta, mode='min', verbose=1)\n",
    "  callback_trainLoss = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=1, min_delta=min_delta, mode='min', verbose=1)\n",
    "\n",
    "  ann_model = tf.keras.Sequential()\n",
    "  limit = numOfLayers\n",
    "\n",
    "  if (len(numOfUnits) is not numOfLayers) and (len(numOfUnits) is not 1):\n",
    "    raise Exception(\"Invalid input for numOfUnits.\")\n",
    "\n",
    "  ann_model.add(tf.keras.layers.Normalization(axis=-1))\n",
    "  \n",
    "  for i in range(0, limit):\n",
    "    if len(numOfUnits) is numOfLayers:\n",
    "      numUnits = numOfUnits[i]\n",
    "    \n",
    "    else:\n",
    "      numUnits = numOfUnits[0]\n",
    "\n",
    "    ann_model.add(tf.keras.layers.Dense(units=numUnits, activation=\"relu\"))\n",
    "  \n",
    "  ann_model.add(tf.keras.layers.Dense(1)) # for 1 value output\n",
    "  ann_model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    loss='mean_squared_error', metrics=['accuracy'])\n",
    "  \n",
    "  ann_model_fit = ann_model.fit(train_features, train_labels, batch_size=batchNum, epochs=epochNum, validation_split=0.3, callbacks=[callback_valLoss, callback_trainLoss])\n",
    "  \n",
    "\n",
    "  return ann_model, ann_model_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6O7Eu1KHJ-OR"
   },
   "source": [
    "###Artificial Neural Network for Using 1 Variable for Predicting the Same Variable Using the Previous Value As a Target- Dump Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1360,
     "status": "ok",
     "timestamp": 1667283834355,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": 240
    },
    "id": "K4t0AHN7KA0F",
    "outputId": "52bcdeef-fa14-46bb-ec30-fafbcfb7463b"
   },
   "outputs": [],
   "source": [
    "# Here we can start building this model first then we move forward with 4 variables models\n",
    "# Restructuring the data such that the target/label is the previous value\n",
    "listOfVariables = [\"HR\"] # Change this to use a different health variable\n",
    "listOfVariables_len = len(listOfVariables) - 1\n",
    "\n",
    "data_train, data_test = split_data_train_test(np.array(case4_filteredData_scaled[listOfVariables]), train_size=0.8) # scaled\n",
    "# data_train, data_test = split_data_train_test(np.array(combinedData_df[listOfVariables]), train_size=0.8) # non-scaled\n",
    "\n",
    "print(\"\\nOriginal data train and test: \")\n",
    "print(data_train[0:20])\n",
    "print(data_test[0:20])\n",
    "\n",
    "data_train_features = []\n",
    "data_train_label = []\n",
    "data_test_features = []\n",
    "data_test_label = []\n",
    "\n",
    "for i in range(1, len(data_train)):\n",
    "  data_train_features.append(data_train[i])\n",
    "  data_train_label.append(data_train[i - 1])\n",
    "\n",
    "for j in range(1, len(data_test)):\n",
    "  data_test_features.append(data_test[j])\n",
    "  data_test_label.append(data_test[j - 1])\n",
    "\n",
    "data_train_features = np.array(data_train_features).reshape(len(data_train_features), 1)\n",
    "data_train_label = np.array(data_train_label).reshape(len(data_train_label), 1)\n",
    "data_test_features = np.array(data_test_features).reshape(len(data_test_features), 1)\n",
    "data_test_label = np.array(data_test_label).reshape(len(data_test_label), 1)\n",
    "\n",
    "print(data_train_features[0:20]), print(data_train_label[0:20]), print(data_test_features[0:20]), print(data_test_label[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 966
    },
    "executionInfo": {
     "elapsed": 372848,
     "status": "ok",
     "timestamp": 1667284210113,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": 240
    },
    "id": "n4hvcKnu4EE5",
    "outputId": "aba6c1ad-b6ab-484e-812e-cbeb8b93bdcb"
   },
   "outputs": [],
   "source": [
    "# Running the model- artificial neural network\n",
    "\n",
    "# batch_num = int(len(data_train_label) / 50)\n",
    "batch_num = None\n",
    "epochNum = 5000\n",
    "learn_rate = 0.000001\n",
    "numOfUnits_list = [512]\n",
    "ann_model, ann_model_fit = make_custom_ANN(data_train_features, data_train_label, numOfUnits=numOfUnits_list, batchNum=batch_num,\n",
    "                                           numOfLayers=3, epochNum=epochNum, learning_rate=learn_rate, min_delta=0.0001)\n",
    "\n",
    "plot_loss(ann_model_fit, 0, epochNum)\n",
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1667284241249,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": 240
    },
    "id": "7ns6ODgnFyBW",
    "outputId": "8de26414-1178-4268-c75c-07d20731e31f"
   },
   "outputs": [],
   "source": [
    "print(data_train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 98966,
     "status": "ok",
     "timestamp": 1667284341113,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": 240
    },
    "id": "RKOgf0704Kk2",
    "outputId": "a4655358-ae0f-4518-8d87-dc5ca858d0bb"
   },
   "outputs": [],
   "source": [
    "# Predicting with the model - training set - ANN\n",
    "\n",
    "print(\"\\nFor training set:\\n\")\n",
    "predictions = ann_model.predict(data_train_features)\n",
    "\n",
    "predictions = predictions.reshape(data_train_features.shape[0], data_train_features.shape[1]) # current shape is (length, numOfPrevValues, 1)\n",
    "predictions = predictions[:, 0] # getting values for the next time step (t) and not (t+1, t+2, or t+3)\n",
    "predictions = np.reshape(predictions, newshape=(len(predictions), 1))\n",
    "# predictions[predictions > 1.0] = 1.0\n",
    "\n",
    "get_and_print_rmse_for_model(data_train_label, predictions)\n",
    "get_and_print_R2_for_model(data_train_label, predictions)\n",
    "get_and_print_mae_for_model(data_train_label, predictions)\n",
    "get_and_print_mape_for_model(data_train_label, predictions)\n",
    "\n",
    "print(\"\\nAfter reversing the scaling:\\n\")\n",
    "\n",
    "# For when MinMaxScaler was used; change the column number/index to get the values for other health variables\n",
    "predictions_reversed = ((predictions - 0.1) / (1 - 0.1)) * (case4_filteredData_max[1] - case4_filteredData_min[1]) + case4_filteredData_min[1]\n",
    "data_train_label_reversed = ((data_train_label - 0.1) / (1 - 0.1)) * (case4_filteredData_max[1] - case4_filteredData_min[1]) + case4_filteredData_min[1]\n",
    "data_train_features_reversed = ((data_train_features - 0.1) / (1 - 0.1)) * (case4_filteredData_max[1] - case4_filteredData_min[1]) + case4_filteredData_min[1]\n",
    "# predictions_reversed[predictions_reversed > 100] = 100\n",
    "\n",
    "# For when standard scaling was used; change the column number/index to get the values for other health variables\n",
    "# predictions_reversed = predictions * combinedData_stdDev[2] + combinedData_mean[2]\n",
    "# data_test_label_reversed = data_test_label * combinedData_stdDev[2] + combinedData_mean[2]\n",
    "# data_test_features_reversed = data_test_features * combinedData_stdDev[0] + combinedData_mean[0]\n",
    "\n",
    "get_and_print_rmse_for_model(data_train_label_reversed, predictions_reversed)\n",
    "get_and_print_R2_for_model(data_train_label_reversed, predictions_reversed)\n",
    "get_and_print_mae_for_model(data_train_label_reversed, predictions_reversed)\n",
    "get_and_print_mape_for_model(data_train_label_reversed, predictions_reversed)\n",
    "\n",
    "plot_actual_and_predictions_line(np.arange(0, len(data_train_label)), data_train_label, predictions)\n",
    "plot_actual_and_predictions_scatter(np.arange(0, len(data_train_label)), data_train_label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 28754,
     "status": "ok",
     "timestamp": 1667284525183,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": 240
    },
    "id": "Gy9qCldA4N9Y",
    "outputId": "fbd0dfeb-5e00-4e7b-96ef-203e43dac263"
   },
   "outputs": [],
   "source": [
    "# Predicting with the model - test set - ANN\n",
    "\n",
    "print(\"\\nFor testing set:\\n\")\n",
    "predictions = ann_model.predict(data_test_features)\n",
    "\n",
    "predictions = predictions.reshape(data_test_features.shape[0], data_test_features.shape[1]) # current shape is (length, numOfPrevValues, 1)\n",
    "predictions = predictions[:, 0] # getting values for the next time step (t) and not (t+1, t+2, or t+3)\n",
    "predictions = np.reshape(predictions, newshape=(len(predictions), 1))\n",
    "# predictions[predictions > 1.0] = 1.0\n",
    "\n",
    "get_and_print_rmse_for_model(data_test_label, predictions)\n",
    "get_and_print_R2_for_model(data_test_label, predictions)\n",
    "get_and_print_mae_for_model(data_test_label, predictions)\n",
    "get_and_print_mape_for_model(data_test_label, predictions)\n",
    "\n",
    "# column 2 for SpO2 values, 0 for ECG R-Wave Voltages; change the column number/index to get the values for other health variables\n",
    "print(\"\\nAfter reversing the scaling: \\n\")\n",
    "\n",
    "# For when MinMaxScaler was used\n",
    "predictions_reversed = ((predictions - 0.1) / (1 - 0.1)) * (case4_filteredData_max[1] - case4_filteredData_min[1]) + case4_filteredData_min[1]\n",
    "data_test_label_reversed = ((data_test_label - 0.1) / (1 - 0.1)) * (case4_filteredData_max[1] - case4_filteredData_min[1]) + case4_filteredData_min[1]\n",
    "data_test_features_reversed = ((data_test_features - 0.1) / (1 - 0.1)) * (case4_filteredData_max[1] - case4_filteredData_min[1]) + case4_filteredData_min[1]\n",
    "# predictions_reversed[predictions_reversed > 100] = 100\n",
    "\n",
    "# For when standard scaling was used; change the column number/index to get the values for other health variables\n",
    "# predictions_reversed = predictions * combinedData_stdDev[2] + combinedData_mean[2]\n",
    "# data_test_label_reversed = data_test_label * combinedData_stdDev[2] + combinedData_mean[2]\n",
    "# data_test_features_reversed = data_test_features * combinedData_stdDev[0] + combinedData_mean[0]\n",
    "\n",
    "get_and_print_rmse_for_model(data_test_label_reversed, predictions_reversed)\n",
    "get_and_print_R2_for_model(data_test_label_reversed, predictions_reversed)\n",
    "get_and_print_mae_for_model(data_test_label_reversed, predictions_reversed)\n",
    "get_and_print_mape_for_model(data_test_label_reversed, predictions_reversed)\n",
    "\n",
    "print(\"\\nRange of Values: \" + str(case4_filteredData_min[2]) + \" to \" + str(case4_filteredData_max[2]) + \"\\n\")\n",
    "\n",
    "plot_actual_and_predictions_line(np.arange(0, len(data_test_label)), data_test_label, predictions)\n",
    "plot_actual_and_predictions_scatter(np.arange(0, len(data_test_label)), data_test_label, predictions)\n",
    "\n",
    "plot_model_data(data_test_features, data_test_label, predictions, \"HR\", \"HR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDzSAjs7ByHy"
   },
   "source": [
    "###Artificial Neural Network Model for Using HR, SpO2, NBP (Mean), ECG to Predict HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 624874,
     "status": "ok",
     "timestamp": 1671541917176,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "Rt6hvm7lCDh7",
    "outputId": "74ad616c-cf0c-41b3-8309-3a39bf26fb0c"
   },
   "outputs": [],
   "source": [
    "# Running the model for HR - artificial neural network\n",
    "\n",
    "data_train_features, data_train_label, data_test_features, data_test_label = splitDataToFeaturesTargetValues(data_train, data_test, numOfPrevValues=27, targetIndex=0)\n",
    "\n",
    "# batch_num = int(len(data_train_label) / 1350)\n",
    "batch_num = None\n",
    "epochNum = 5000\n",
    "learn_rate = 0.000001\n",
    "numOfUnits_list = [512]\n",
    "print(\"SAJDAS\")\n",
    "\n",
    "\n",
    "\n",
    "ann_model_hr, ann_model_hr_fit = make_custom_ANN(data_train_features, data_train_label, numOfUnits=numOfUnits_list, batchNum=batch_num,\n",
    "                                           numOfLayers=3, epochNum=epochNum, learning_rate=learn_rate, min_delta=0.0001)\n",
    "\n",
    "plot_loss(ann_model_hr_fit, 0, epochNum)\n",
    "ann_model_hr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1671541940531,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "IQHUSHuqV9Gj",
    "outputId": "6d1e8b3d-aff0-47ff-e9b9-fd4a25b4998a"
   },
   "outputs": [],
   "source": [
    "print(data_train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 39614,
     "status": "ok",
     "timestamp": 1671542538226,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "ix3R-ePeUdlz",
    "outputId": "7bac02f2-e015-48ce-d6e4-39e19de85d30"
   },
   "outputs": [],
   "source": [
    "# Predicting with the model for HR - training set - ANN\n",
    "\n",
    "print(\"\\nFor training set:\\n\")\n",
    "predictions = ann_model_hr.predict(data_train_features)\n",
    "\n",
    "predictions = predictions.reshape(data_train_features.shape[0], data_train_features.shape[1]) # current shape is (length, numOfPrevValues, 1)\n",
    "predictions = predictions[:, 0] # getting values for the next time step (t) and not (t+1, t+2, or t+3)\n",
    "predictions = np.reshape(predictions, newshape=(len(predictions), 1))\n",
    "\n",
    "get_and_print_rmse_for_model(data_train_label, predictions)\n",
    "get_and_print_R2_for_model(data_train_label, predictions)\n",
    "get_and_print_mae_for_model(data_train_label, predictions)\n",
    "get_and_print_mape_for_model(data_train_label, predictions)\n",
    "\n",
    "plot_actual_and_predictions_line(np.arange(0, len(data_train_label)), data_train_label, predictions)\n",
    "plot_actual_and_predictions_scatter(np.arange(0, len(data_train_label)), data_train_label, predictions)\n",
    "\n",
    "print(\"\\nAfter reversing the scaling:\\n\")\n",
    "\n",
    "# For if MinMaxScaler was used; change the column number/index to get the values for other health variables\n",
    "predictions_reversed = ((predictions - 0.1) / (1 - 0.1)) * (case4_filteredData_max[1] - case4_filteredData_min[1]) + case4_filteredData_min[1]\n",
    "data_train_label_reversed = ((data_train_label - 0.1) / (1 - 0.1)) * (case4_filteredData_max[1] - case4_filteredData_min[1]) + case4_filteredData_min[1]\n",
    "# data_train_features_reversed = ((data_train_features - 0.1) / (1 - 0.1)) * (combinedData_max[0] - combinedData_min[0]) + combinedData_min[0]\n",
    "\n",
    "# For if standard scaling was used; change the column number/index to get the values for other health variables\n",
    "# predictions_reversed = predictions * combinedData_stdDev[1] + combinedData_mean[1]\n",
    "# data_train_label_reversed = data_train_label * combinedData_stdDev[1] + combinedData_mean[1]\n",
    "# data_train_features_reversed = data_train_features * combinedData_stdDev[0] + combinedData_mean[0]\n",
    "\n",
    "get_and_print_rmse_for_model(data_train_label_reversed, predictions_reversed)\n",
    "get_and_print_R2_for_model(data_train_label_reversed, predictions_reversed)\n",
    "get_and_print_mae_for_model(data_train_label_reversed, predictions_reversed)\n",
    "get_and_print_mape_for_model(data_train_label_reversed, predictions_reversed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9269,
     "status": "ok",
     "timestamp": 1671542558363,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "QgTzdYf4Yjbt",
    "outputId": "57cc08d4-71d8-498f-bc6f-3630b4db6062"
   },
   "outputs": [],
   "source": [
    "# Predicting with the model for HR - test set - ANN\n",
    "\n",
    "print(\"\\nFor testing set:\\n\")\n",
    "predictions_test = ann_model_hr.predict(data_test_features)\n",
    "\n",
    "predictions_test = predictions_test.reshape(data_test_features.shape[0], data_test_features.shape[1]) # current shape is (length, numOfPrevValues, 1)\n",
    "predictions_test = predictions_test[:, 0] # getting values for the next time step (t) and not (t+1, t+2, or t+3)\n",
    "predictions_test = np.reshape(predictions_test, newshape=(len(predictions_test), 1))\n",
    "# predictions[predictions > 1.0] = 1.0\n",
    "\n",
    "get_and_print_rmse_for_model(data_test_label, predictions_test)\n",
    "get_and_print_R2_for_model(data_test_label, predictions_test)\n",
    "get_and_print_mae_for_model(data_test_label, predictions_test)\n",
    "get_and_print_mape_for_model(data_test_label, predictions_test)\n",
    "\n",
    "plot_actual_and_predictions_line(np.arange(0, len(data_test_label)), data_test_label, predictions_test)\n",
    "plot_actual_and_predictions_scatter(np.arange(0, len(data_test_label)), data_test_label, predictions_test)\n",
    "\n",
    "# # this function won't plot properly if there are more than 3 variables in the model\n",
    "# plot_model_data3D(data_test_features[:, :, 0][:, 0].flatten(), data_test_features[:, :, 2][:, 0].flatten(), data_test_features[:, :, 1][:, 0].flatten(),\n",
    "#                  predictions.flatten(), \"R-Wave Voltages\", \"SpO2\", \"HR\") \n",
    "\n",
    "# column 1 for HR values, 0 for ECG R-Wave Voltages\n",
    "print(\"\\nAfter reversing the scaling: \\n\")\n",
    "\n",
    "# For if MinMaxScaler was used; change the column number/index to get the values for other health variables\n",
    "predictions_reversed = ((predictions_test - 0.1) / (1 - 0.1)) * (case4_filteredData_max[0] - case4_filteredData_max[1]) + case4_filteredData_max[1]\n",
    "data_test_label_reversed = ((data_test_label - 0.1) / (1 - 0.1)) * (case4_filteredData_max[0] - case4_filteredData_max[1]) + case4_filteredData_max[1]\n",
    "# data_test_features_reversed = ((data_test_features - 0.1) / (1 - 0.1)) * (combinedData_max[0] - combinedData_min[0]) + combinedData_min[0]\n",
    "# predictions_reversed[predictions_reversed > 100] = 100\n",
    "\n",
    "# For if standard scaling was used; change the column number/index to get the values for other health variables\n",
    "# predictions_reversed = predictions * combinedData_stdDev[1] + combinedData_mean[1]\n",
    "# data_test_label_reversed = data_test_label * combinedData_stdDev[1] + combinedData_mean[1]\n",
    "# data_test_features_reversed = data_test_features * combinedData_stdDev[0] + combinedData_mean[0]\n",
    "\n",
    "get_and_print_rmse_for_model(data_test_label_reversed, predictions_reversed)\n",
    "get_and_print_R2_for_model(data_test_label_reversed, predictions_reversed)\n",
    "get_and_print_mae_for_model(data_test_label_reversed, predictions_reversed)\n",
    "get_and_print_mape_for_model(data_test_label_reversed, predictions_reversed)\n",
    "\n",
    "print(\"DATA TEST\")\n",
    "print(data_test_label)\n",
    "print(\"this is the predictions\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "executionInfo": {
     "elapsed": 1432,
     "status": "ok",
     "timestamp": 1671542569485,
     "user": {
      "displayName": "Khaled Saleh",
      "userId": "01251108770465165621"
     },
     "user_tz": -120
    },
    "id": "EWsQrBMVsJ8N",
    "outputId": "825768e9-68b0-4a79-f0d3-b6ece26994db"
   },
   "outputs": [],
   "source": [
    "\n",
    "#HR\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "xtrain_set = np.arange(0, len(data_train_features))\n",
    "ytrain_set = np.array(data_train_label[:,0])\n",
    "plt.plot(xtrain_set, ytrain_set, 'b', label=\"Actual Train\")\n",
    "\n",
    "xtest_set = np.arange(len(data_train_features),len(data_train_features) + len(data_test_features))\n",
    "ytest_set = np.array(data_test_label[:,0])\n",
    "plt.plot(xtest_set, ytest_set, 'r', label=\"Actual Test\")\n",
    "\n",
    "pred_train_sig = np.array(predictions[:,0])\n",
    "plt.plot(xtrain_set, pred_train_sig, 'orange', label=\"Predicted Train\", linestyle=\"dotted\")\n",
    "\n",
    "pred_test_sig = np.array(predictions_test[:,0])\n",
    "plt.plot(xtest_set, pred_test_sig,'green', label=\"Predicted Test\", linestyle=\"dotted\")\n",
    "plt.title(\"Heart Rate (HR)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
