{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NilFsyfHxi7"
   },
   "source": [
    "# Early Stage 3 Variable LSTM Model\n",
    "Purpose:\n",
    "- experimented with both ANN and LSTM models at the beginning of the term to determine model with best results\n",
    "- utilizes many-to-one prediction method\n",
    "\n",
    "Notes:\n",
    "- based off of the 3 variable model from the Summer 2022 term\n",
    "- does not implement downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7UPkRBNHIfA"
   },
   "source": [
    "# Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17986,
     "status": "ok",
     "timestamp": 1671568301541,
     "user": {
      "displayName": "Joanne Jo",
      "userId": "01049188753505025865"
     },
     "user_tz": 300
    },
    "id": "J3K_y_kSHRGa",
    "outputId": "e1ee4097-f539-43a8-cb1a-faab228628cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wfdb\n",
      "  Downloading wfdb-4.1.0-py3-none-any.whl (159 kB)\n",
      "     -------------------------------------- 159.9/159.9 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (1.4.4)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.2.2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (3.5.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.8.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (2.28.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (1.9.1)\n",
      "Collecting SoundFile<0.12.0,>=0.10.0\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.10.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from wfdb) (1.21.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from matplotlib<4.0.0,>=3.2.2->wfdb) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from pandas<2.0.0,>=1.0.0->wfdb) (2022.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.8.1->wfdb) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.8.1->wfdb) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.8.1->wfdb) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.8.1->wfdb) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from SoundFile<0.12.0,>=0.10.0->wfdb) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from cffi>=1.0->SoundFile<0.12.0,>=0.10.0->wfdb) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.2.2->wfdb) (1.16.0)\n",
      "Installing collected packages: SoundFile, wfdb\n",
      "Successfully installed SoundFile-0.11.0 wfdb-4.1.0\n",
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.19.0-cp39-cp39-win_amd64.whl (742 kB)\n",
      "     -------------------------------------- 742.5/742.5 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorflow_addons) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from packaging->tensorflow_addons) (3.0.9)\n",
      "Installing collected packages: typeguard, tensorflow_addons\n",
      "Successfully installed tensorflow_addons-0.19.0 typeguard-2.13.3\n",
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.1.3-py3-none-any.whl (135 kB)\n",
      "     -------------------------------------- 135.7/135.7 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from keras-tuner) (21.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from keras-tuner) (1.21.5)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: ipython in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from keras-tuner) (7.31.1)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from keras-tuner) (2.28.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.1.6)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: pygments in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (2.11.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.4.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (63.4.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from ipython->keras-tuner) (3.0.20)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from packaging->keras-tuner) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (0.37.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (2.0.3)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "     -------------------------------------- 124.6/124.6 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "     ------------------------------------- 177.0/177.0 kB 10.4 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 6.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from tensorboard->keras-tuner) (3.3.4)\n",
      "Collecting protobuf<4,>=3.9.2\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-win_amd64.whl (904 kB)\n",
      "     -------------------------------------- 904.2/904.2 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (1.16.0)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython->keras-tuner) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\vithu\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, kt-legacy, tensorboard-data-server, rsa, protobuf, oauthlib, grpcio, cachetools, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, keras-tuner\n",
      "Successfully installed absl-py-1.3.0 cachetools-5.2.1 google-auth-2.15.0 google-auth-oauthlib-0.4.6 grpcio-1.51.1 keras-tuner-1.1.3 kt-legacy-1.0.4 oauthlib-3.2.2 protobuf-3.20.3 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1\n"
     ]
    }
   ],
   "source": [
    "# install external libraries\n",
    "\n",
    "!pip install wfdb\n",
    "!pip install tensorflow_addons\n",
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTqz91gxHdw1"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import time\n",
    "import wfdb\n",
    "from wfdb import processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03vueH3tH5Fu"
   },
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69119,
     "status": "ok",
     "timestamp": 1671568375584,
     "user": {
      "displayName": "Joanne Jo",
      "userId": "01049188753505025865"
     },
     "user_tz": 300
    },
    "id": "KMxJ5r2kH7VY",
    "outputId": "baecb58a-e8e4-44b4-c53d-de1091aa588b"
   },
   "outputs": [],
   "source": [
    "# connect to Google Drive account\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5443,
     "status": "ok",
     "timestamp": 1671568381008,
     "user": {
      "displayName": "Joanne Jo",
      "userId": "01049188753505025865"
     },
     "user_tz": 300
    },
    "id": "HQIs4xT6ICHn",
    "outputId": "cf6eb473-66d3-49e3-e864-18acd18190d6"
   },
   "outputs": [],
   "source": [
    "# Changing directory to the csv dataset needed\n",
    "os.chdir(\"/content/drive/MyDrive/Mobile Health/Machine Learning/Ready to use Datasets /uqvitalsignsdata/case04/fulldata\")\n",
    "os.listdir()\n",
    "\n",
    "# loading data - move file into folders so you don't mount drive every time\n",
    "df = pd.read_csv(\"uq_vsd_case04_alldata.csv\", index_col=False, usecols=['HR', 'SpO2', 'NBP (Mean)', 'ECG'], error_bad_lines=False) # issue with getting time column as index column?\n",
    "print(df.head(10))\n",
    "print(df.tail(10))\n",
    "print(df.shape)\n",
    "\n",
    "case4_filteredData = df.loc[(~df['HR'].isnull()) & (~df['SpO2'].isnull()) & (~df['NBP (Mean)'].isnull()) & (~df['ECG'].isnull())]\n",
    "print(case4_filteredData.head(10))\n",
    "print(case4_filteredData.tail(10))\n",
    "print(case4_filteredData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1671568381743,
     "user": {
      "displayName": "Joanne Jo",
      "userId": "01049188753505025865"
     },
     "user_tz": 300
    },
    "id": "FNxCaL5kK-h5",
    "outputId": "ca368b3a-ec54-47ce-954c-fc9359037592"
   },
   "outputs": [],
   "source": [
    "# # plot data\n",
    "# dataset = pd.read_csv(\"new_data.csv\", index_col=0, header=0)\n",
    "values = case4_filteredData.values\n",
    "\n",
    "# specify columns to plot\n",
    "groups = [0,1,2,3]\n",
    "i =1\n",
    "for g in groups:\n",
    "  plt.subplot(len(groups), 1, i)\n",
    "  plt.plot(values[:, g])\n",
    "  plt.title(case4_filteredData.columns[g], y=0.5, loc='right')\n",
    "  i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4aGmepCNNX5"
   },
   "source": [
    "# Preprocessing/Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eh8hW8HUNqDV"
   },
   "outputs": [],
   "source": [
    "# Function for splitting data into training and test set non-randomly\n",
    "\n",
    "\"\"\"\n",
    "This functions splits the data into training and test sets.\n",
    "\n",
    "@param data_df : a Pandas Dataframe that contains the data to be split into training and test sets\n",
    "@param train_size : a double/float that is within the range of 0 and 1 and represents the fraction of the data to be used for the training set\n",
    "\n",
    "@returns two Numpy arrays that represent the training and test sets, respectively\n",
    "\"\"\"\n",
    "def split_data_train_test(data_df, train_size):\n",
    "  train_df, test_df = data_df[0:round(train_size * len(data_df)), :], data_df[round(train_size * len(data_df)):len(data_df), :]\n",
    "\n",
    "  return train_df, test_df\n",
    "\n",
    "# Function for scaling the data\n",
    "\"\"\"\n",
    "This function takes in the data to be scaled and uses Min-Max Scaling.\n",
    "\n",
    "@param data : a Numpy array that represents the inputted data to be scaled\n",
    "\n",
    "@returns the scaled data as a Numpy array, the MinMax scaler object, the scaling factor used, and the minimum value of the original inputted data\n",
    "\"\"\"\n",
    "def minMaxScaling(data):\n",
    "  scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0.1, 1))\n",
    "  scaler.fit(data)\n",
    "\n",
    "  scaleFactor = scaler.scale_\n",
    "  data_min = scaler.data_min_\n",
    "  data_max = scaler.data_max_\n",
    "  data = scaler.transform(data)\n",
    "\n",
    "  return data, scaler, scaleFactor, data_min, data_max\n",
    "\n",
    "\"\"\"\n",
    "This function takes in the data to be scaled and uses Standard Scaling.\n",
    "\n",
    "@param data : a Numpy array that represents the inputted data to be scaled\n",
    "\n",
    "@returns the scaled data as a Numpy array, the Standard scaler object, the mean of the original inputted data, and the standard deviation of the original inputted data\n",
    "\"\"\"\n",
    "def stdScaling(data):\n",
    "  scaler = sklearn.preprocessing.StandardScaler()\n",
    "  scaler.fit(data)\n",
    "\n",
    "  mean = scaler.mean_\n",
    "  std_dev = scaler.var_ ** 0.5\n",
    "  data = scaler.transform(data)\n",
    "\n",
    "  return data, scaler, mean, std_dev\n",
    "\n",
    "\"\"\"\n",
    "This function plots the inputted data in a 2D scatterplot.\n",
    "\n",
    "@param xvals : a 1D Numpy array that contains the data to be plotted on the x-axis\n",
    "@param data : a 1D Numpy array that contains the data to be plotted on the y-axis\n",
    "@param xVariable : a string that represents the label of the data to be plotted for x-axis\n",
    "@param yVariable : a string that represents the label of the data to be plotted for y-axis\n",
    "\"\"\"\n",
    "def plot_data(xvals, data, xVariable, yVariable):\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.scatter(xvals, data)\n",
    "  plt.title(\"Plot of Data for \" + yVariable + \", \" + xVariable)\n",
    "  plt.ylabel(yVariable)\n",
    "  plt.xlabel(xVariable)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1671568381749,
     "user": {
      "displayName": "Joanne Jo",
      "userId": "01049188753505025865"
     },
     "user_tz": 300
    },
    "id": "Xn6Ck7DPPej5",
    "outputId": "294f0c0c-91fc-468f-8269-487f675414d6"
   },
   "outputs": [],
   "source": [
    "# Scaling and splitting dataset, using R-Wave Voltages and SpO2 as features\n",
    "pd.set_option(\"max_rows\", 1000)\n",
    "print(case4_filteredData.head(20))\n",
    "\n",
    "case4_filteredData_arr_scaled, minMaxScaler, minMaxScaleFactor, case4_filteredData_min, case4_filteredData_max = minMaxScaling(case4_filteredData)\n",
    "# combinedData_arr_scaled, stdScaler, combinedData_mean, combinedData_stdDev = stdScaling(combinedData_df) # uncomment this if standard scaling will be used\n",
    "\n",
    "# Add more columns names if more health variables are added.\n",
    "case4_filteredData_df_scaled = pd.DataFrame(case4_filteredData_arr_scaled, columns=[\"SpO2\", \"HR\", \"NBP (Mean)\", \"ECG\"])\n",
    "\n",
    "print()\n",
    "print(case4_filteredData_df_scaled[0:20])\n",
    "\n",
    "listOfVariables = [\"SpO2\", \"HR\", \"NBP (Mean)\", \"ECG\"] # Add more columns names if more health variables are added.\n",
    "listOfVariables_len = len(listOfVariables) - 1 \n",
    "\n",
    "data_train, data_test = split_data_train_test(np.array(case4_filteredData_df_scaled[listOfVariables]), train_size=0.8) # scaled data\n",
    "# data_train, data_test = split_data_train_test(np.array(combinedData_df[listOfVariables]), train_size=0.8) # non-scaled\n",
    "\n",
    "print(\"\\nOriginal data train and test: \")\n",
    "print(data_train[0:20])\n",
    "print(data_test[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1436,
     "status": "ok",
     "timestamp": 1671568383166,
     "user": {
      "displayName": "Joanne Jo",
      "userId": "01049188753505025865"
     },
     "user_tz": 300
    },
    "id": "IxcNqAhrP6u0",
    "outputId": "83a98e3f-f368-47ac-b020-ee429ed587cb"
   },
   "outputs": [],
   "source": [
    "# Splitting data into previous values and next values\n",
    "\n",
    "\"\"\"\n",
    "This function splits the training and test sets and turns a certain number of previous values as \"features\" and the next value as the \"target\"\n",
    "\n",
    "@param train_array : a Numpy array with at least 2 columns of the training set values\n",
    "@param test_array : a Numpy array with at least 2 columns of the test set values\n",
    "@param numOfPrevValues : an integer that represents the number of previous rows of values to use as features\n",
    "@param targetIndex : an integer that represents the column number to use for target values\n",
    "\n",
    "@returns four Numpy arrays for the previous values in the training set, the next 1 value in the training set, the previous values\n",
    "         in the test set, and the next 1 value in the test set\n",
    "\"\"\"\n",
    "def splitDataToFeaturesTargetValues(train_array, test_array, numOfPrevValues=2, targetIndex=1):\n",
    "\n",
    "  prevData_train = [] # contains at least 2 columns of data, like ECG R-Wave Voltages and SpO2\n",
    "  nextData_train = [] # contains only 1 column of data, like SpO2\n",
    "  prevData_test = []\n",
    "  nextData_test = []\n",
    "\n",
    "  for i in range(numOfPrevValues, len(train_array)):\n",
    "    prevData_train.append(train_array[i-numOfPrevValues: i])\n",
    "    nextData_train.append(train_array[i][targetIndex])\n",
    "\n",
    "  for j in range(numOfPrevValues, len(test_array)):\n",
    "    prevData_test.append(test_array[j-numOfPrevValues: j])\n",
    "    nextData_test.append(test_array[j][targetIndex])\n",
    "  \n",
    "  prevData_train = np.array(prevData_train)\n",
    "  nextData_train = np.array(nextData_train).reshape(len(nextData_train), 1)\n",
    "  prevData_test = np.array(prevData_test)\n",
    "  nextData_test = np.array(nextData_test).reshape(len(nextData_test), 1)\n",
    "\n",
    "  return prevData_train, nextData_train, prevData_test, nextData_test\n",
    "\n",
    "data_train_features, data_train_label, data_test_features, data_test_label = splitDataToFeaturesTargetValues(data_train, data_test, numOfPrevValues=50, targetIndex=2)\n",
    "\n",
    "print(data_train_features[0:10])\n",
    "print(data_train_label[0:10])\n",
    "print(data_test_features[0:10])\n",
    "print(data_test_label[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9N1L2BfKLsQ7"
   },
   "source": [
    "# General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IFzraJ0Lvkq"
   },
   "outputs": [],
   "source": [
    "# Function that makes the loss function plots and linear regression model\n",
    "\n",
    "\"\"\"\n",
    "This function plots a loss function based on the output of the model fitting.\n",
    "\n",
    "@param loss_vals : a Tensorflow History object from model fitting, which is used to get the training and validation loss values\n",
    "@param startRange : an integer that is >= 0, is within the indices of loss_vals, and represents the start indices of the loss values to plot\n",
    "@param endRange : an integer that is <= len(loss_vals) - 1, is larger than startRange, and represents the end indices of the loss values to plot\n",
    "\"\"\"\n",
    "def plot_loss(loss_vals, startRange, endRange):\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plt.plot(loss_vals.history['loss'][startRange:endRange], label=\"Training Loss\") \n",
    "  plt.plot(loss_vals.history['val_loss'][startRange:endRange], label=\"Validation Loss\") \n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Error\")\n",
    "  plt.title(\"Learning Curve\")\n",
    "  plt.legend()\n",
    "\n",
    "\"\"\"\n",
    "This function plots the inputted actual and predicted data in a 2D scatterplot.\n",
    "\n",
    "@param xvals : a 1D Numpy array that contains the data to be plotted on the x-axis\n",
    "@param actual_data : a 1D Numpy array that contains the actual data to be plotted on the y-axis\n",
    "@param predicted_data : a 1D Numpy array that contains the predicted data to be plotted on the y-axis\n",
    "@param xName : a string that represents the x-values and label for the x-axis\n",
    "@param yName : a string that represents the y-values and label for the y-axis\n",
    "\"\"\"\n",
    "def plot_model_data(xvals, actual_data, predicted_data, xName, yName):\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plt.scatter(xvals, actual_data, label=\"Actual Data\", color=\"r\")\n",
    "  plt.plot(xvals, predicted_data, label=\"Predicted Data\", color=\"black\")\n",
    "  plt.title(\"Plot of Model Data for \" + xName + \" Against \" + yName)\n",
    "  plt.xlabel(xName)\n",
    "  plt.ylabel(yName)\n",
    "  plt.legend()\n",
    "\n",
    "\"\"\"\n",
    "This function plots the inputted actual and predicted data in a 3D scatterplot.\n",
    "\n",
    "@param x_val : a 1D Numpy array that contains the data to be plotted on the x-axis\n",
    "@param y_val : a 1D Numpy array that contains the data to be plotted on the y-axis\n",
    "@param actual_data : a 1D Numpy array that contains the actual data to be plotted on the z-axis\n",
    "@param predicted_data : a 1D Numpy array that contains the predicted data to be plotted on the z-axis\n",
    "@param xName : a string that represents the x-values and label for the x-axis\n",
    "@param yName : a string that represents the y-values and label for the y-axis\n",
    "@param zName : a string that represents the z-values and label for the z-axis\n",
    "\"\"\"\n",
    "def plot_model_data3D(x_val, y_val, actual_data, predicted_data, xName, yName, zName):\n",
    "  fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "  ax = plt.axes(projection='3d')\n",
    "  ax.scatter3D(x_val, y_val, actual_data, label='Actual Data', color=\"red\")\n",
    "  ax.plot3D(x_val, y_val, predicted_data, label='Predicted Data', color=\"black\")\n",
    "\n",
    "  ax.set_title(\"Plot of Model Data for \" + xName + \" And \" + yName + \" Against \" + zName, pad=20)\n",
    "  ax.set_xlabel(xName)\n",
    "  ax.set_ylabel(yName)\n",
    "  ax.set_zlabel(zName)\n",
    "  ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypsqnYklMAvA"
   },
   "outputs": [],
   "source": [
    "# Functions for getting metrics and plotting predictions\n",
    "\n",
    "\"\"\"\n",
    "This functions gets and prints the root mean squared error (RMSE) of the targets.\n",
    "\n",
    "@param test_targets : a Numpy array that represents the actual target values\n",
    "@param predicted_targets : a Numpy array that represents the predicted target values\n",
    "\n",
    "@returns the RMSE double/float value\n",
    "\"\"\"\n",
    "def get_and_print_rmse_for_model(test_targets, predicted_targets):\n",
    "  rmseMetric = tf.keras.metrics.RootMeanSquaredError()\n",
    "  rmseMetric.update_state([test_targets], [predicted_targets])\n",
    "  rmseMetricResult = rmseMetric.result().numpy()\n",
    "\n",
    "  print(\"RMSE: \" + str(rmseMetricResult))\n",
    "  return rmseMetricResult\n",
    "\n",
    "\"\"\"\n",
    "This functions gets and prints the coefficient of determination (R^2) of the targets.\n",
    "\n",
    "@param test_targets : a Numpy array that represents the actual target values\n",
    "@param predicted_targets : a Numpy array that represents the predicted target values\n",
    "\n",
    "@returns the R^2 double/float value\n",
    "\"\"\"\n",
    "def get_and_print_R2_for_model(test_targets, predicted_targets):\n",
    "  r2 = sklearn.metrics.r2_score(y_true=test_targets, y_pred=predicted_targets)\n",
    "  print(\"R2 (Sklearn): \" + str(r2))\n",
    "  \n",
    "  return r2\n",
    "\n",
    "\"\"\"\n",
    "This function uses the mean absolute percentage error (MAPE) to get a percentage error for a regression model. \n",
    "\n",
    "Note that if the output is a number above 1 or below 0, usually very big in magnitude, it is because \n",
    "some of the numbers in the test_targets are 0 or very close to 0, which gives a division by 0.\n",
    "\n",
    "@param test_targets : a Numpy array that represents the actual target values\n",
    "@param predicted_targets : a Numpy array that represents the predicted target values\n",
    "\n",
    "@returns the MAPE double/float value which represents the decimal form of the percentage, so generally between 0 and 1\n",
    "\"\"\"\n",
    "def get_and_print_mape_for_model(test_targets, predicted_targets):\n",
    "  mape = sklearn.metrics.mean_absolute_percentage_error(test_targets, predicted_targets)\n",
    "  print(\"MAPE: \" + str(mape))\n",
    "\n",
    "  return mape\n",
    "\n",
    "\"\"\"\n",
    "This function gets the mean absolute error (MAE) for a regression model. \n",
    "\n",
    "Note that if the output is a number below 0, usually very big in magnitude, it is because \n",
    "some of the numbers in the test_targets are 0 or very close to 0, which gives a division by 0.\n",
    "\n",
    "@param test_targets : a Numpy array that represents the actual target values\n",
    "@param predicted_targets : a Numpy array that represents the predicted target values\n",
    "\n",
    "@returns the MAPE double/float value, which should be greater than or equal to 0\n",
    "\"\"\"\n",
    "def get_and_print_mae_for_model(test_targets, predicted_targets):\n",
    "  mae = sklearn.metrics.mean_absolute_error(test_targets, predicted_targets)\n",
    "  print(\"MAE: \" + str(mae))\n",
    "\n",
    "  return mae\n",
    "\n",
    "\"\"\"\n",
    "This functions plots the actual and predicted values as a line plot.\n",
    "\n",
    "@param indices : the x-values of the plot as a 1-column Numpy array\n",
    "@param test_targets : a 1-column Numpy array that represents the actual target values\n",
    "@param predicted_targets : a 1-column Numpy array that represents the predicted target values\n",
    "\"\"\"\n",
    "def plot_actual_and_predictions_line(x_val, test_targets, predicted_targets):\n",
    "  fig = plt.figure(figsize=(12, 8))\n",
    "  plt.plot(x_val, test_targets, label=\"Actual Values\", c=\"b\")\n",
    "  plt.plot(x_val, predicted_targets, label=\"Predicted Values\", c=\"r\")\n",
    "\n",
    "  plt.ylabel(\"Values\")\n",
    "  plt.title(\"Plot of Actual and Predicted Values\")\n",
    "  plt.legend()\n",
    "\n",
    "\"\"\"\n",
    "This functions plots the actual and predicted values as a scatter plot.\n",
    "\n",
    "@param indices : a 1-column Numpy array that represents x-values of the plot \n",
    "@param test_targets : a 1-column Numpy array that represents the actual target values\n",
    "@param predicted_targets : a 1-column Numpy array that represents the predicted target values\n",
    "\"\"\"\n",
    "def plot_actual_and_predictions_scatter(x_val, test_targets, predicted_targets):\n",
    "  fig = plt.figure(figsize=(12, 8))\n",
    "  plt.scatter(x_val, test_targets, label=\"Actual Values\", c=\"b\")\n",
    "  plt.scatter(x_val, predicted_targets, label=\"Predicted Values\", c=\"r\")\n",
    "\n",
    "  plt.ylabel(\"Value\")\n",
    "  plt.title(\"Plot of Actual and Predicted Values\")\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1671568383167,
     "user": {
      "displayName": "Joanne Jo",
      "userId": "01049188753505025865"
     },
     "user_tz": 300
    },
    "id": "QAn0lDzgMGVj",
    "outputId": "bef4c463-133e-436b-aa97-31500d94e96a"
   },
   "outputs": [],
   "source": [
    "# Functions for making different ML models\n",
    "\n",
    "# Function for making LSTM model with a certain amount of layers\n",
    "\"\"\"\n",
    "This function makes a custom LSTM neural network model.\n",
    "\n",
    "@param train_features : a Numpy array that represents the training data features\n",
    "@param train_labels : a Numpy array that represents the training data targets\n",
    "@param numOfUnits : a list that contains the number of units for each layer, which could be customized by providing different integers in the desired order. \n",
    "                    The size of the list should either be equal to 1 or numOfLayers. If size = 1, then all layers will have that number of units in the list.\n",
    "                    (eg. [4, 8, 16] if numOfLayers = 3, or [4] for any value of numOfLayers)\n",
    "@param numOfLayers : an integer that represents the desired of layers to add to the model \n",
    "@param epochNum : the number of iterations/epochs for running the model\n",
    "@param learning_rate : a double/float that represents the learning rate of the model\n",
    "\n",
    "@returns the LSTM neural network TensorFlow model and the result from fitting the model\n",
    "\n",
    "\"\"\"\n",
    "def make_custom_LSTM(train_features, train_labels, numOfUnits=[4], numOfLayers=1, batchNum=None, epochNum=100, learning_rate=0.01, min_delta=0.0007):\n",
    "  callback_valLoss = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, min_delta=min_delta, mode='min', verbose=1)\n",
    "  callback_trainLoss = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=1, min_delta=min_delta, mode='min', verbose=1)\n",
    "  \n",
    "  lstm_model = tf.keras.Sequential()\n",
    "  limit = numOfLayers\n",
    "\n",
    "  if (len(numOfUnits) is not numOfLayers) and (len(numOfUnits) is not 1):\n",
    "    raise Exception(\"Invalid input for numOfUnits.\")\n",
    "\n",
    "  lstm_model.add(tf.keras.layers.Normalization(axis=-1))\n",
    "  \n",
    "  for i in range(0, limit):\n",
    "    if len(numOfUnits) is numOfLayers:\n",
    "      numUnits = numOfUnits[i]\n",
    "    \n",
    "    else:\n",
    "      numUnits = numOfUnits[0]\n",
    "\n",
    "    lstm_model.add(tf.keras.layers.LSTM(units=numUnits, return_sequences=True))\n",
    "  \n",
    "  lstm_model.add(tf.keras.layers.Dense(1)) # for 1 value output\n",
    "  lstm_model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    loss='mean_squared_error', metrics=['accuracy'])\n",
    "  \n",
    "  lstm_model_fit = lstm_model.fit(train_features, train_labels, batch_size=batchNum, epochs=epochNum, validation_split=0.3, callbacks=[callback_valLoss, callback_trainLoss])\n",
    "\n",
    "  return lstm_model, lstm_model_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3byvRu2gMhdt"
   },
   "source": [
    "# LSTM Model - Predicting HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2612820,
     "status": "ok",
     "timestamp": 1671570995983,
     "user": {
      "displayName": "Joanne Jo",
      "userId": "01049188753505025865"
     },
     "user_tz": 300
    },
    "id": "-NT6eItLMnoZ",
    "outputId": "294acdf1-6e7f-44f1-ce38-3b8e55aa5a9d"
   },
   "outputs": [],
   "source": [
    "# Running the models to predict SpO2 - LSTM\n",
    "\n",
    "data_train_features, data_train_label, data_test_features, data_test_label = splitDataToFeaturesTargetValues(data_train, data_test, numOfPrevValues=2, targetIndex=0)\n",
    "print(data_train_features.shape)\n",
    "\n",
    "batch_num = int(len(data_train_label) / 1550)\n",
    "# batch_num = None\n",
    "epochNum = 1000\n",
    "learn_rate = 0.0001\n",
    "numOfUnits_list = [512]\n",
    "\n",
    "print(\"\\nBatch Size: \" + str(batch_num) + \"\\n\")\n",
    "\n",
    "lstm_model_spo2, lstm_model_spo2_fit = make_custom_LSTM(data_train_features, data_train_label, numOfUnits=numOfUnits_list, batchNum=batch_num,\n",
    "                                              numOfLayers=6, epochNum=epochNum, learning_rate=learn_rate, min_delta=0.0001)\n",
    "\n",
    "plot_loss(lstm_model_spo2_fit, 0, epochNum)\n",
    "lstm_model_spo2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1001868,
     "status": "ok",
     "timestamp": 1671571997843,
     "user": {
      "displayName": "Joanne Jo",
      "userId": "01049188753505025865"
     },
     "user_tz": 300
    },
    "id": "gg0W_FpkMqf_",
    "outputId": "81f36deb-d7ea-402b-c9b8-abb613a6041d"
   },
   "outputs": [],
   "source": [
    "# Predicting with the model for HR - training set - LSTM\n",
    "\n",
    "print(\"\\nFor training set:\\n\")\n",
    "predictions = lstm_model_spo2.predict(data_train_features)\n",
    "\n",
    "predictions = predictions.reshape(data_train_features.shape[0], data_train_features.shape[1]) # current shape is (length, numOfPrevValues, 1)\n",
    "predictions = predictions[:, 0] # getting values for the next time step (t) and not (t+1, t+2, or t+3)\n",
    "predictions = np.reshape(predictions, newshape=(len(predictions), 1))\n",
    "\n",
    "get_and_print_rmse_for_model(data_train_label, predictions)\n",
    "get_and_print_R2_for_model(data_train_label, predictions)\n",
    "get_and_print_mae_for_model(data_train_label, predictions)\n",
    "get_and_print_mape_for_model(data_train_label, predictions)\n",
    "\n",
    "print(\"\\nAfter reversing the scaling:\\n\")\n",
    "\n",
    "# For if MinMaxScaler was used; change the column number/index to get the values for other health variables\n",
    "predictions_reversed = ((predictions - 0.1) / (1 - 0.1)) * (case4_filteredData_max[0] - case4_filteredData_min[0]) + case4_filteredData_min[0]\n",
    "data_train_label_reversed = ((data_train_label - 0.1) / (1 - 0.1)) * (case4_filteredData_max[0] - case4_filteredData_min[0]) + case4_filteredData_min[0]\n",
    "# data_train_features_reversed = ((data_train_features - 0.1) / (1 - 0.1)) * (combinedData_max[0] - combinedData_min[0]) + combinedData_min[0]\n",
    "\n",
    "# For if standard scaling was used; change the column number/index to get the values for other health variables\n",
    "# predictions_reversed = predictions * combinedData_stdDev[2] + combinedData_mean[2]\n",
    "# data_train_label_reversed = data_train_label * combinedData_stdDev[2] + combinedData_mean[2]\n",
    "# data_train_features_reversed = data_train_features * combinedData_stdDev[0] + combinedData_mean[0]\n",
    "\n",
    "get_and_print_rmse_for_model(data_train_label_reversed, predictions_reversed)\n",
    "get_and_print_R2_for_model(data_train_label_reversed, predictions_reversed)\n",
    "get_and_print_mae_for_model(data_train_label_reversed, predictions_reversed)\n",
    "get_and_print_mape_for_model(data_train_label_reversed, predictions_reversed)\n",
    "\n",
    "plot_actual_and_predictions_line(np.arange(0, len(data_train_label)), data_train_label, predictions)\n",
    "plot_actual_and_predictions_scatter(np.arange(0, len(data_train_label)), data_train_label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 364055,
     "status": "ok",
     "timestamp": 1671572361880,
     "user": {
      "displayName": "Joanne Jo",
      "userId": "01049188753505025865"
     },
     "user_tz": 300
    },
    "id": "UEZD4pGTMtDZ",
    "outputId": "8ee44cfe-9f12-4b95-c5aa-25abed164669"
   },
   "outputs": [],
   "source": [
    "# Predicting with the model for HR- test set - LSTM\n",
    "\n",
    "print(\"\\nFor testing set:\\n\")\n",
    "predictions = lstm_model_spo2.predict(data_test_features)\n",
    "\n",
    "predictions = predictions.reshape(data_test_features.shape[0], data_test_features.shape[1]) # current shape is (length, numOfPrevValues, 1)\n",
    "predictions = predictions[:, 0] # getting values for the next time step (t) and not (t+1, t+2, or t+3)\n",
    "predictions = np.reshape(predictions, newshape=(len(predictions), 1))\n",
    "\n",
    "get_and_print_rmse_for_model(data_test_label, predictions)\n",
    "get_and_print_R2_for_model(data_test_label, predictions)\n",
    "get_and_print_mae_for_model(data_test_label, predictions)\n",
    "get_and_print_mape_for_model(data_test_label, predictions)\n",
    "\n",
    "plot_actual_and_predictions_line(np.arange(0, len(data_test_label)), data_test_label, predictions)\n",
    "plot_actual_and_predictions_scatter(np.arange(0, len(data_test_label)), data_test_label, predictions)\n",
    "\n",
    "# this function won't plot properly if there are more than 3 variables in the model\n",
    "# plot_model_data3D(data_test_features[:, :, 0][:, 0].flatten(), data_test_features[:, :, 1][:, 0].flatten(), data_test_features[:, :, 2][:, 0].flatten(),\n",
    "                  # predictions.flatten(), \"SpO2\", \"HR\", \"NBP (Mean)\", \"ECG\")\n",
    "\n",
    "# column 2 for SpO2 values, 0 for ECG R-Wave Voltages; change the column number/index to get the values for other health variables\n",
    "print(\"\\nAfter reversing the scaling: \\n\")\n",
    "\n",
    "# For if MinMaxScaler was used\n",
    "predictions_reversed = ((predictions - 0.1) / (1 - 0.1)) * (case4_filteredData_max[0] - case4_filteredData_min[0]) + case4_filteredData_min[0]\n",
    "data_test_label_reversed = ((data_test_label - 0.1) / (1 - 0.1)) * (case4_filteredData_max[0] - case4_filteredData_min[0]) + case4_filteredData_min[0]\n",
    "# data_test_features_reversed = ((data_test_features - 0.1) / (1 - 0.1)) * (combinedData_max[0] - combinedData_min[0]) + combinedData_min[0]\n",
    "\n",
    "# For if standard scaling was used; change the column number/index to get the values for other health variables\n",
    "# predictions_reversed = predictions * combinedData_stdDev[2] + combinedData_mean[2]\n",
    "# data_test_label_reversed = data_test_label * combinedData_stdDev[2] + combinedData_mean[2]\n",
    "# data_test_features_reversed = data_test_features * combinedData_stdDev[0] + combinedData_mean[0]\n",
    "\n",
    "get_and_print_rmse_for_model(data_test_label_reversed, predictions_reversed)\n",
    "get_and_print_R2_for_model(data_test_label_reversed, predictions_reversed)\n",
    "get_and_print_mae_for_model(data_test_label_reversed, predictions_reversed)\n",
    "get_and_print_mape_for_model(data_test_label_reversed, predictions_reversed)\n",
    "\n",
    "print(\"\\nRange of Values: \" + str(case4_filteredData_min[2]) + \" to \" + str(case4_filteredData_max[2]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "r7UPkRBNHIfA",
    "03vueH3tH5Fu"
   ],
   "provenance": [
    {
     "file_id": "1VRJHGvB9nWdVZcQwkbLea1YsuGJBYcxw",
     "timestamp": 1671579981456
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
