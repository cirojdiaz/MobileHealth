{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dgl\n",
    "import createGraph\n",
    "from importlib import reload\n",
    "import torch\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import architecture\n",
    "import training_loop\n",
    "from bayes_opt import BayesianOptimization\n",
    "from torch.utils.data import DataLoader \n",
    "from bayes_opt.util import UtilityFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new indices to 2nd graph\n",
    "def offset_node_batches(node_batches, offset):\n",
    "    return [[idx + offset for idx in batch] for batch in node_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in graphs directly from binary files\n",
    "train_g, train_labels_dict = dgl.load_graphs('upsampledTrain.bin')\n",
    "val_g, val_labels_dict = dgl.load_graphs('nonupsampledTest.bin')\n",
    "\n",
    "# Assign labels \n",
    "train_y = train_labels_dict['gLabel']\n",
    "val_y = val_labels_dict['gLabel']\n",
    "\n",
    "# Make a list of cumulative sums of nodes for each graph in training and val set\n",
    "train_cumsum_nodes = np.cumsum([g.number_of_nodes() for g in train_g])\n",
    "val_cumsum_nodes = np.cumsum([g.number_of_nodes() for g in val_g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists containg indices of nodes for each graph in training and val set\n",
    "train_node_batches = [list(range(train_cumsum_nodes[i - 1] if i > 0 else 0, train_cumsum_nodes[i])) for i in range(len(train_g))]\n",
    "val_node_batches = [list(range(val_cumsum_nodes[i - 1] if i > 0 else 0, val_cumsum_nodes[i])) for i in range(len(val_g))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last indexed value from train_cumsum_nodes is total number of nodes in training set\n",
    "num_train_nodes = train_cumsum_nodes[-1]\n",
    "# Add number of train nodes to each idx in val set \n",
    "offset_val_node_batches = offset_node_batches(val_node_batches, num_train_nodes)\n",
    "# Combine into single graph\n",
    "combined_g = dgl.batch(train_g + val_g)\n",
    "# Get total number of batches/indices \n",
    "combined_node_batches = train_node_batches + offset_val_node_batches\n",
    "# Combine labels for train and val into single tensor\n",
    "combined_y = torch.cat([train_y, val_y])\n",
    "# Mask to select training set\n",
    "train_mask = list(range(len(train_y)))\n",
    "# Mask to select val set\n",
    "val_mask = list(range(len(train_y), len(train_y) + len(val_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | dropou... | l1_lambda | lamb_b... | lamb_b... | lamb_eps  |  lamb_wd  | lookah... | lookah... |    lr     | max_ep... | num_ba... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Roaming\\Python\\Python311\\site-packages\\dgl\\backend\\pytorch\\tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scores in iteration:  {'epoch': 7, 'train_loss': 0.6838671565055847, 'train_acc': 0.5764989256858826, 'train_bal_acc': 0.5751935543789697, 'train_recall': 0.6452344931921331, 'train_precision': 0.57508848811731, 'train_f1': 0.6081454415827466, 'train_auc': 0.5751935543789697, 'val_loss': 0.6808741092681885, 'val_acc': 0.5611045956611633, 'val_bal_acc': 0.5049006942788244, 'val_recall': 0.6427378964941569, 'val_precision': 0.7070707070707071, 'val_f1': 0.6733712286838653, 'val_auc': 0.5049006942788246}\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.5049   \u001b[0m | \u001b[0m0.3962   \u001b[0m | \u001b[0m-4.544   \u001b[0m | \u001b[0m0.8508   \u001b[0m | \u001b[0m0.8789   \u001b[0m | \u001b[0m-6.491   \u001b[0m | \u001b[0m-1.014   \u001b[0m | \u001b[0m0.4633   \u001b[0m | \u001b[0m8.403    \u001b[0m | \u001b[0m-1.578   \u001b[0m | \u001b[0m65.5     \u001b[0m | \u001b[0m43.97    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 9, 'train_loss': 0.688248872756958, 'train_acc': 0.5011317133903503, 'train_bal_acc': 0.5034049820718229, 'train_recall': 0.38142965204236007, 'train_precision': 0.5138198955547064, 'train_f1': 0.4378357844467357, 'train_auc': 0.5034049820718229, 'val_loss': 0.693512499332428, 'val_acc': 0.4218566417694092, 'val_bal_acc': 0.43816910459230995, 'val_recall': 0.3981636060100167, 'val_precision': 0.6445945945945946, 'val_f1': 0.49226006191950467, 'val_auc': 0.4381691045923099}\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.4382   \u001b[0m | \u001b[0m0.2821   \u001b[0m | \u001b[0m-4.709   \u001b[0m | \u001b[0m0.9936   \u001b[0m | \u001b[0m0.9809   \u001b[0m | \u001b[0m-5.76    \u001b[0m | \u001b[0m-3.746   \u001b[0m | \u001b[0m0.6553   \u001b[0m | \u001b[0m7.722    \u001b[0m | \u001b[0m-3.24    \u001b[0m | \u001b[0m75.36    \u001b[0m | \u001b[0m65.99    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 0, 'train_loss': 0.6875764727592468, 'train_acc': 0.499494343996048, 'train_bal_acc': 0.49792788739866445, 'train_recall': 0.5819780635400907, 'train_precision': 0.5075451471922157, 'train_f1': 0.5422190899881072, 'train_auc': 0.49792788739866445, 'val_loss': 0.6811360120773315, 'val_acc': 0.5352526307106018, 'val_bal_acc': 0.5083754008002757, 'val_recall': 0.5742904841402338, 'val_precision': 0.7100103199174407, 'val_f1': 0.6349792339640056, 'val_auc': 0.5083754008002755}\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.5084   \u001b[0m | \u001b[95m0.7425   \u001b[0m | \u001b[95m-3.857   \u001b[0m | \u001b[95m0.933    \u001b[0m | \u001b[95m0.8104   \u001b[0m | \u001b[95m-6.692   \u001b[0m | \u001b[95m-4.774   \u001b[0m | \u001b[95m0.3719   \u001b[0m | \u001b[95m9.482    \u001b[0m | \u001b[95m-0.7789  \u001b[0m | \u001b[95m85.7     \u001b[0m | \u001b[95m91.85    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 1, 'train_loss': 0.6850821375846863, 'train_acc': 0.5259812474250793, 'train_bal_acc': 0.519528954945182, 'train_recall': 0.8657337367624811, 'train_precision': 0.5208487399738324, 'train_f1': 0.6503995737879595, 'train_auc': 0.5195289549451821, 'val_loss': 0.6735851168632507, 'val_acc': 0.6445358395576477, 'val_bal_acc': 0.4992248986405915, 'val_recall': 0.8555926544240401, 'val_precision': 0.7035003431708992, 'val_f1': 0.7721280602636534, 'val_auc': 0.4992248986405915}\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.4992   \u001b[0m | \u001b[0m0.3699   \u001b[0m | \u001b[0m-3.137   \u001b[0m | \u001b[0m0.8809   \u001b[0m | \u001b[0m0.936    \u001b[0m | \u001b[0m-5.4     \u001b[0m | \u001b[0m-2.605   \u001b[0m | \u001b[0m0.3888   \u001b[0m | \u001b[0m7.777    \u001b[0m | \u001b[0m-1.049   \u001b[0m | \u001b[0m89.84    \u001b[0m | \u001b[0m54.57    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 9, 'train_loss': 0.6912028193473816, 'train_acc': 0.4944377541542053, 'train_bal_acc': 0.5013029368980727, 'train_recall': 0.13294251134644477, 'train_precision': 0.5142648134601316, 'train_f1': 0.21126972201352368, 'train_auc': 0.5013029368980727, 'val_loss': 0.7050309777259827, 'val_acc': 0.35076379776000977, 'val_bal_acc': 0.49686150197418977, 'val_recall': 0.13856427378964942, 'val_precision': 0.694560669456067, 'val_f1': 0.23103688239387615, 'val_auc': 0.4968615019741897}\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.4969   \u001b[0m | \u001b[0m0.7344   \u001b[0m | \u001b[0m-3.436   \u001b[0m | \u001b[0m0.9444   \u001b[0m | \u001b[0m0.8248   \u001b[0m | \u001b[0m-4.335   \u001b[0m | \u001b[0m-3.449   \u001b[0m | \u001b[0m0.418    \u001b[0m | \u001b[0m7.317    \u001b[0m | \u001b[0m-2.895   \u001b[0m | \u001b[0m62.77    \u001b[0m | \u001b[0m105.0    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 0, 'train_loss': 0.691849410533905, 'train_acc': 0.5002648830413818, 'train_bal_acc': 0.49418435313126535, 'train_recall': 0.8204425113464447, 'train_precision': 0.5058000582920431, 'train_f1': 0.6257978435685694, 'train_auc': 0.49418435313126535, 'val_loss': 0.6675213575363159, 'val_acc': 0.6715628504753113, 'val_bal_acc': 0.5626755571455071, 'val_recall': 0.8297161936560935, 'val_precision': 0.7368421052631579, 'val_f1': 0.7805261091480173, 'val_auc': 0.5626755571455071}\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.5627   \u001b[0m | \u001b[95m0.1437   \u001b[0m | \u001b[95m-4.546   \u001b[0m | \u001b[95m0.9313   \u001b[0m | \u001b[95m0.8953   \u001b[0m | \u001b[95m-7.706   \u001b[0m | \u001b[95m-4.444   \u001b[0m | \u001b[95m0.3449   \u001b[0m | \u001b[95m6.344    \u001b[0m | \u001b[95m-2.569   \u001b[0m | \u001b[95m97.9     \u001b[0m | \u001b[95m73.39    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 9, 'train_loss': 0.686428964138031, 'train_acc': 0.4906814396381378, 'train_bal_acc': 0.49999640864810896, 'train_recall': 0.00018910741301059002, 'train_precision': 0.5, 'train_f1': 0.0003780718336483932, 'train_auc': 0.4999964086481089, 'val_loss': 0.7119814157485962, 'val_acc': 0.2961222231388092, 'val_bal_acc': 0.5, 'val_recall': 0.0, 'val_precision': 0.0, 'val_f1': 0.0, 'val_auc': 0.5}\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m0.4218   \u001b[0m | \u001b[0m-3.155   \u001b[0m | \u001b[0m0.9829   \u001b[0m | \u001b[0m0.8501   \u001b[0m | \u001b[0m-5.269   \u001b[0m | \u001b[0m-1.143   \u001b[0m | \u001b[0m0.3908   \u001b[0m | \u001b[0m8.012    \u001b[0m | \u001b[0m-1.41    \u001b[0m | \u001b[0m96.84    \u001b[0m | \u001b[0m106.9    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 9, 'train_loss': 0.6786121129989624, 'train_acc': 0.6000481843948364, 'train_bal_acc': 0.6017057631565232, 'train_recall': 0.5127647503782148, 'train_precision': 0.6324198250728863, 'train_f1': 0.56634118322803, 'train_auc': 0.6017057631565232, 'val_loss': 0.6865388751029968, 'val_acc': 0.48002350330352783, 'val_bal_acc': 0.4639710363833903, 'val_recall': 0.503338898163606, 'val_precision': 0.6752519596864501, 'val_f1': 0.5767575322812051, 'val_auc': 0.4639710363833903}\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.464    \u001b[0m | \u001b[0m0.3378   \u001b[0m | \u001b[0m-4.989   \u001b[0m | \u001b[0m0.8069   \u001b[0m | \u001b[0m0.8028   \u001b[0m | \u001b[0m-5.644   \u001b[0m | \u001b[0m-3.468   \u001b[0m | \u001b[0m0.3459   \u001b[0m | \u001b[0m9.051    \u001b[0m | \u001b[0m-1.303   \u001b[0m | \u001b[0m86.87    \u001b[0m | \u001b[0m105.2    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 0, 'train_loss': 0.6849657893180847, 'train_acc': 0.5279075503349304, 'train_bal_acc': 0.5247545993060951, 'train_recall': 0.69392965204236, 'train_precision': 0.5277957569219706, 'train_f1': 0.5995670111515053, 'train_auc': 0.5247545993060951, 'val_loss': 0.6805748343467712, 'val_acc': 0.5569917559623718, 'val_bal_acc': 0.4824393168508361, 'val_recall': 0.6652754590984975, 'val_precision': 0.6930434782608695, 'val_f1': 0.6788756388415672, 'val_auc': 0.4824393168508361}\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.4824   \u001b[0m | \u001b[0m0.3218   \u001b[0m | \u001b[0m-3.503   \u001b[0m | \u001b[0m0.9114   \u001b[0m | \u001b[0m0.922    \u001b[0m | \u001b[0m-6.808   \u001b[0m | \u001b[0m-2.59    \u001b[0m | \u001b[0m0.4703   \u001b[0m | \u001b[0m6.77     \u001b[0m | \u001b[0m-2.81    \u001b[0m | \u001b[0m74.99    \u001b[0m | \u001b[0m114.8    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 9, 'train_loss': 0.6847606301307678, 'train_acc': 0.5296412110328674, 'train_bal_acc': 0.5325277463949509, 'train_recall': 0.3776475037821483, 'train_precision': 0.5563448948321493, 'train_f1': 0.44990143621515066, 'train_auc': 0.5325277463949509, 'val_loss': 0.6888414621353149, 'val_acc': 0.43419507145881653, 'val_bal_acc': 0.47854227946047645, 'val_recall': 0.36978297161936563, 'val_precision': 0.6804915514592934, 'val_f1': 0.4791779340183884, 'val_auc': 0.4785422794604765}\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.4785   \u001b[0m | \u001b[0m0.211    \u001b[0m | \u001b[0m-4.901   \u001b[0m | \u001b[0m0.8921   \u001b[0m | \u001b[0m0.8812   \u001b[0m | \u001b[0m-6.052   \u001b[0m | \u001b[0m-4.718   \u001b[0m | \u001b[0m0.5328   \u001b[0m | \u001b[0m9.909    \u001b[0m | \u001b[0m-4.059   \u001b[0m | \u001b[0m60.62    \u001b[0m | \u001b[0m120.2    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 0, 'train_loss': 0.6888611316680908, 'train_acc': 0.5055622458457947, 'train_bal_acc': 0.4973341450592872, 'train_recall': 0.9388237518910741, 'train_precision': 0.5079032175558852, 'train_f1': 0.6591867219917013, 'train_auc': 0.4973341450592872, 'val_loss': 0.663007378578186, 'val_acc': 0.6698002219200134, 'val_bal_acc': 0.49303402496223864, 'val_recall': 0.9265442404006677, 'val_precision': 0.7007575757575758, 'val_f1': 0.7979870596693027, 'val_auc': 0.49303402496223864}\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.493    \u001b[0m | \u001b[0m0.5659   \u001b[0m | \u001b[0m-3.814   \u001b[0m | \u001b[0m0.8758   \u001b[0m | \u001b[0m0.9271   \u001b[0m | \u001b[0m-5.99    \u001b[0m | \u001b[0m-4.641   \u001b[0m | \u001b[0m0.4189   \u001b[0m | \u001b[0m8.797    \u001b[0m | \u001b[0m-4.21    \u001b[0m | \u001b[0m95.22    \u001b[0m | \u001b[0m73.95    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 0, 'train_loss': 0.6901681423187256, 'train_acc': 0.46048638224601746, 'train_bal_acc': 0.4585741874561706, 'train_recall': 0.5611762481089259, 'train_precision': 0.4749139793550452, 'train_f1': 0.5144541238677242, 'train_auc': 0.45857418745617073, 'val_loss': 0.6825106143951416, 'val_acc': 0.5258519649505615, 'val_bal_acc': 0.4873300739327451, 'val_recall': 0.5818030050083473, 'val_precision': 0.6949152542372882, 'val_f1': 0.6333484779645616, 'val_auc': 0.4873300739327451}\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.4873   \u001b[0m | \u001b[0m0.01421  \u001b[0m | \u001b[0m-4.26    \u001b[0m | \u001b[0m0.8084   \u001b[0m | \u001b[0m0.8253   \u001b[0m | \u001b[0m-7.283   \u001b[0m | \u001b[0m-4.334   \u001b[0m | \u001b[0m0.6535   \u001b[0m | \u001b[0m9.342    \u001b[0m | \u001b[0m-3.941   \u001b[0m | \u001b[0m70.36    \u001b[0m | \u001b[0m59.42    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 0, 'train_loss': 0.6939261555671692, 'train_acc': 0.5094630122184753, 'train_bal_acc': 0.5001903138102867, 'train_recall': 0.9977307110438729, 'train_precision': 0.5094139229506613, 'train_f1': 0.6744646852029402, 'train_auc': 0.5001903138102866, 'val_loss': 0.6476697325706482, 'val_acc': 0.7021151781082153, 'val_bal_acc': 0.4993226144102605, 'val_recall': 0.996661101836394, 'val_precision': 0.703594578668238, 'val_f1': 0.8248704663212435, 'val_auc': 0.4993226144102605}\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.4993   \u001b[0m | \u001b[0m0.6644   \u001b[0m | \u001b[0m-4.416   \u001b[0m | \u001b[0m0.8346   \u001b[0m | \u001b[0m0.8912   \u001b[0m | \u001b[0m-4.39    \u001b[0m | \u001b[0m-2.62    \u001b[0m | \u001b[0m0.465    \u001b[0m | \u001b[0m5.84     \u001b[0m | \u001b[0m-3.612   \u001b[0m | \u001b[0m65.89    \u001b[0m | \u001b[0m52.47    \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 0, 'train_loss': 0.6874704360961914, 'train_acc': 0.5138935446739197, 'train_bal_acc': 0.5053855476798642, 'train_recall': 0.9618948562783661, 'train_precision': 0.5121325010068466, 'train_f1': 0.6683968462549278, 'train_auc': 0.5053855476798642, 'val_loss': 0.662095308303833, 'val_acc': 0.6897767186164856, 'val_bal_acc': 0.500902628719824, 'val_recall': 0.9641068447412354, 'val_precision': 0.7042682926829268, 'val_f1': 0.813953488372093, 'val_auc': 0.500902628719824}\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.5009   \u001b[0m | \u001b[0m0.5109   \u001b[0m | \u001b[0m-3.409   \u001b[0m | \u001b[0m0.8074   \u001b[0m | \u001b[0m0.9462   \u001b[0m | \u001b[0m-4.91    \u001b[0m | \u001b[0m-4.022   \u001b[0m | \u001b[0m0.694    \u001b[0m | \u001b[0m7.725    \u001b[0m | \u001b[0m-0.9826  \u001b[0m | \u001b[0m62.15    \u001b[0m | \u001b[0m7.41     \u001b[0m |\n",
      "Best scores in iteration:  {'epoch': 9, 'train_loss': 0.6924600005149841, 'train_acc': 0.4927040636539459, 'train_bal_acc': 0.5007053108874645, 'train_recall': 0.07138804841149773, 'train_precision': 0.5143051771117166, 'train_f1': 0.1253736300232481, 'train_auc': 0.5007053108874644, 'val_loss': 0.7243750691413879, 'val_acc': 0.31198590993881226, 'val_bal_acc': 0.48713132999443515, 'val_recall': 0.05759599332220367, 'val_precision': 0.6216216216216216, 'val_f1': 0.10542398777692895, 'val_auc': 0.48713132999443515}\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.4871   \u001b[0m | \u001b[0m0.5681   \u001b[0m | \u001b[0m-3.085   \u001b[0m | \u001b[0m0.8045   \u001b[0m | \u001b[0m0.9817   \u001b[0m | \u001b[0m-5.087   \u001b[0m | \u001b[0m-1.178   \u001b[0m | \u001b[0m0.4093   \u001b[0m | \u001b[0m4.194    \u001b[0m | \u001b[0m-3.548   \u001b[0m | \u001b[0m63.86    \u001b[0m | \u001b[0m61.83    \u001b[0m |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m optimizer\u001b[39m.\u001b[39mset_gp_params(normalize_y\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, alpha\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[39m# Maximize validation accuracy performing 20 steps of random exploration and 10 steps of baysian optimization\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m optimizer\u001b[39m.\u001b[39;49mmaximize(init_points\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, acquisition_function\u001b[39m=\u001b[39;49mutility)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bayes_opt\\bayesian_optimization.py:310\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m     x_probe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuggest(util)\n\u001b[0;32m    309\u001b[0m     iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 310\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobe(x_probe, lazy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    312\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer \u001b[39mand\u001b[39;00m iteration \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    313\u001b[0m     \u001b[39m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_bounds(\n\u001b[0;32m    316\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bayes_opt\\bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue\u001b[39m.\u001b[39madd(params)\n\u001b[0;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_space\u001b[39m.\u001b[39;49mprobe(params)\n\u001b[0;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch(Events\u001b[39m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\bayes_opt\\target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    234\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_as_array(params)\n\u001b[0;32m    235\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys, x))\n\u001b[1;32m--> 236\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constraint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mtrain_with_hyperparams\u001b[1;34m(lr, num_batches, max_epochs, lamb_beta1, lamb_beta2, lamb_eps, lamb_wd, lookahead_k, lookahead_alpha, dropout_rate, l1_lambda)\u001b[0m\n\u001b[0;32m     14\u001b[0m lookahead_k \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(lookahead_k)\n\u001b[0;32m     15\u001b[0m model \u001b[39m=\u001b[39m architecture\u001b[39m.\u001b[39mGCN(combined_g\u001b[39m.\u001b[39mndata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m30\u001b[39m, \u001b[39m30\u001b[39m, dropout_rate)\n\u001b[1;32m---> 17\u001b[0m best_val_bal_acc \u001b[39m=\u001b[39m training_loop\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m     18\u001b[0m     g\u001b[39m=\u001b[39;49mcombined_g, \n\u001b[0;32m     19\u001b[0m     node_batches\u001b[39m=\u001b[39;49mcombined_node_batches, \n\u001b[0;32m     20\u001b[0m     model\u001b[39m=\u001b[39;49mmodel, \n\u001b[0;32m     21\u001b[0m     labels\u001b[39m=\u001b[39;49mcombined_y, \n\u001b[0;32m     22\u001b[0m     train_mask\u001b[39m=\u001b[39;49mtrain_mask, \n\u001b[0;32m     23\u001b[0m     val_mask\u001b[39m=\u001b[39;49mval_mask, \n\u001b[0;32m     24\u001b[0m     validate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[0;32m     25\u001b[0m     test\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[0;32m     26\u001b[0m     init_lr\u001b[39m=\u001b[39;49mlr, \n\u001b[0;32m     27\u001b[0m     num_batches\u001b[39m=\u001b[39;49mnum_batches, \n\u001b[0;32m     28\u001b[0m     max_epochs\u001b[39m=\u001b[39;49mmax_epochs,\n\u001b[0;32m     29\u001b[0m     lamb_beta1\u001b[39m=\u001b[39;49mlamb_beta1,\n\u001b[0;32m     30\u001b[0m     lamb_beta2\u001b[39m=\u001b[39;49mlamb_beta2,\n\u001b[0;32m     31\u001b[0m     lamb_eps\u001b[39m=\u001b[39;49mlamb_eps,\n\u001b[0;32m     32\u001b[0m     lamb_wd\u001b[39m=\u001b[39;49mlamb_wd,\n\u001b[0;32m     33\u001b[0m     lookahead_k\u001b[39m=\u001b[39;49mlookahead_k, \n\u001b[0;32m     34\u001b[0m     lookahead_alpha\u001b[39m=\u001b[39;49mlookahead_alpha,\n\u001b[0;32m     35\u001b[0m     l1_lambda\u001b[39m=\u001b[39;49ml1_lambda\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[39mreturn\u001b[39;00m best_val_bal_acc\n",
      "File \u001b[1;32mc:\\Users\\james\\Documents\\2023coop\\MHA_Summer_Dev\\GNN\\training_loop.py:82\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(g, node_batches, model, labels, train_mask, val_mask, test_mask, validate, test, init_lr, stoch, num_batches, early_stopping, early_stopping_patience, early_stopping_warmup, max_epochs, lamb_beta1, lamb_beta2, lamb_eps, lamb_wd, lookahead_k, lookahead_alpha, l1_lambda)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m#logit_multiplier = torch.tensor([1, 1])\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[0;32m     81\u001b[0m     \u001b[39m# iterate through batches\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     logits \u001b[39m=\u001b[39m model(g, features, node_batches)\n\u001b[0;32m     84\u001b[0m     \u001b[39m#new_logits = logits * logit_multiplier\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[0;32m     86\u001b[0m     \u001b[39m#pred = new_logits.argmax(1)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     pred \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39msigmoid(logits) \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\james\\Documents\\2023coop\\MHA_Summer_Dev\\GNN\\architecture.py:169\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, g, node_feats, node_batches)\u001b[0m\n\u001b[0;32m    166\u001b[0m h \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(h)\n\u001b[0;32m    168\u001b[0m \u001b[39m# pool rows belonging to same patient to get patient embeddings\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mavg_pool(h, node_batches)\n\u001b[0;32m    171\u001b[0m \u001b[39m# Upsample h and corresponding labels for it(Need to differentiate using backprop), use a smote function to create synthetic samples so that it only happens in the train set\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[39m# pass patient embeddings through dense layers\u001b[39;00m\n\u001b[0;32m    175\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_1(h)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\james\\Documents\\2023coop\\MHA_Summer_Dev\\GNN\\architecture.py:139\u001b[0m, in \u001b[0;36mAvgPoolingLayer.forward\u001b[1;34m(self, feats, node_batches)\u001b[0m\n\u001b[0;32m    137\u001b[0m pooled_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty(\u001b[39m0\u001b[39m, feats\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m    138\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m node_batches:\n\u001b[1;32m--> 139\u001b[0m     pooled_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([pooled_vals, torch\u001b[39m.\u001b[39;49mmean(feats[batch], dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)], dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m \u001b[39mreturn\u001b[39;00m pooled_vals\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import architecture\n",
    "import training_loop\n",
    "reload(architecture)\n",
    "reload(training_loop)\n",
    "\n",
    "def train_with_hyperparams(lr, num_batches, max_epochs, lamb_beta1, lamb_beta2, lamb_eps, lamb_wd, lookahead_k, lookahead_alpha,dropout_rate,l1_lambda):\n",
    "    # Format hyperparameters \n",
    "    lr = 10**lr\n",
    "    lamb_eps = 10**lamb_eps\n",
    "    lamb_wd = 10**lamb_wd\n",
    "    l1_lambda = 10**l1_lambda\n",
    "    num_batches = int(num_batches)\n",
    "    max_epochs = int(max_epochs)\n",
    "    lookahead_k = int(lookahead_k)\n",
    "    model = architecture.GCN(combined_g.ndata['feat'].shape[1], 30, 30, dropout_rate)\n",
    "\n",
    "    best_val_bal_acc = training_loop.train(\n",
    "        g=combined_g, \n",
    "        node_batches=combined_node_batches, \n",
    "        model=model, \n",
    "        labels=combined_y, \n",
    "        train_mask=train_mask, \n",
    "        val_mask=val_mask, \n",
    "        validate=True, \n",
    "        test=False, \n",
    "        init_lr=lr, \n",
    "        num_batches=num_batches, \n",
    "        max_epochs=max_epochs,\n",
    "        lamb_beta1=lamb_beta1,\n",
    "        lamb_beta2=lamb_beta2,\n",
    "        lamb_eps=lamb_eps,\n",
    "        lamb_wd=lamb_wd,\n",
    "        lookahead_k=lookahead_k, \n",
    "        lookahead_alpha=lookahead_alpha,\n",
    "        l1_lambda=l1_lambda\n",
    "    )\n",
    "    \n",
    "    return best_val_bal_acc\n",
    "\n",
    "\n",
    "hyperparams_bounds = {\n",
    "    'lr': (-5, -0.5), # learning rate, controls convergence to loss function minimum\n",
    "    'num_batches': (1, 125), # number of batches training set is divided into\n",
    "    'max_epochs': (50, 100), # maximum number of passes through the data set the model goes through\n",
    "    'lamb_beta1': (0.8, 0.999), # controls decay rate of first moment estimate\n",
    "    'lamb_beta2': (0.8, 0.999), # controls decay rate of second moment estimate\n",
    "    'lamb_eps': (-8, -4), # small constant added to denominator for numerical stability\n",
    "    'lamb_wd': (-5, -1), # weight decay to prevent overfitting\n",
    "    'lookahead_k': (3, 10), # number of steps of slow weight compared faster weight\n",
    "    'lookahead_alpha': (0.3, 0.7), # slow weight step size\n",
    "    'dropout_rate': (0.001, 0.8), # dropout rate\n",
    "    'l1_lambda':(-5,-3) # l1 regularization coefficient\n",
    "}\n",
    "# Initialize bayesian optimization\n",
    "optimizer = BayesianOptimization(f=train_with_hyperparams, pbounds=hyperparams_bounds, random_state=50)\n",
    "# Set acquisition function as Expected Improvement where a larger 'xi' values encourages more exploration vs. exploitation \n",
    "utility = UtilityFunction(kind=\"ei\", xi=0.01)\n",
    "# Set parameters for Gaussian Process\n",
    "optimizer.set_gp_params(normalize_y=True, alpha=1e-6)\n",
    "# Maximize validation accuracy performing 20 steps of random exploration and 10 steps of baysian optimization\n",
    "optimizer.maximize(init_points=20, n_iter=10, acquisition_function=utility)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(architecture)\n",
    "reload(training_loop)\n",
    "\n",
    "model = architecture.GCN(combined_g.ndata['feat'].shape[1], 30, 30)\n",
    "\n",
    "training_loop.train(g=combined_g, node_batches=combined_node_batches, model=model, labels=combined_y, train_mask=train_mask, val_mask=val_mask, validate=True, test=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
