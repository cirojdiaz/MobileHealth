{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "throughData() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdgl\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcreateGraph\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimportlib\u001b[39;00m \u001b[39mimport\u001b[39;00m reload\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/MHA_Summer_Dev_Local/MHA_Summer_Dev/GNN/createGraph.py:142\u001b[0m\n\u001b[1;32m    139\u001b[0m     csv \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mduke_imputed_normalized_dict.npy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m     throughData(csv)\n\u001b[0;32m--> 142\u001b[0m main()\n",
      "File \u001b[0;32m~/MHA_Summer_Dev_Local/MHA_Summer_Dev/GNN/createGraph.py:140\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m    139\u001b[0m     csv \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mduke_imputed_normalized_dict.npy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 140\u001b[0m     throughData(csv)\n",
      "\u001b[0;31mTypeError\u001b[0m: throughData() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dgl\n",
    "import createGraph\n",
    "from importlib import reload\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import architecture\n",
    "import training_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('duke_vital_model_imputed.csv')\n",
    "X = df.drop(labels=[\"PostCond\"], axis=1).to_numpy()\n",
    "y = df[\"PostCond\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values - process will be made a little more complex later when we impute train and test vals separately; for now we can do it like this\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# convert imputed ndarrays back into a Dataframe\n",
    "imputed_X_df = pd.DataFrame(data=X, columns=df.drop(labels=[\"PostCond\"], axis=1).columns)\n",
    "y_df = pd.DataFrame(data=y, columns=[\"PostCond\"])\n",
    "imputed_df = pd.concat([imputed_X_df, y_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_graphs - figure out how to do this\n",
    "g, y, node_batches = createGraph.throughData(imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/val sets and test sets\n",
    "skf = StratifiedKFold(n_splits = 10, shuffle=True, random_state=42)\n",
    "indices = skf.split(np.zeros(len(y)), y)\n",
    "train_val_mask = dict()\n",
    "test_mask = dict()\n",
    "cnt = 0\n",
    "for train_i, test_i in indices:\n",
    "    train_val_mask[cnt] = train_i\n",
    "    test_mask[cnt] = test_i\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/val sets into train sets and val sets\n",
    "train_mask = dict()\n",
    "val_mask = dict()\n",
    "\n",
    "for i in range(0, 10):\n",
    "    train_mask[i], val_mask[i] = train_test_split(train_val_mask[i], y[train_val_mask[i]], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = architecture.GCN(g['feats'].shape(-1), 30, 30)\n",
    "\n",
    "training_loop.train(g=g, node_batches=node_batches, labels=y, train_mask=train_mask[0], val_mask=val_mask[0], validate=True, test=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09f61af4f65d4634c17b8a51ddf1eb855b373cdae4e53182aef1e6aee687f5e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
